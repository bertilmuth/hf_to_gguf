{"cells":[{"cell_type":"markdown","metadata":{"id":"1oHFCsV0z-Jw"},"source":["# Finetune Llama-3 with LLaMA Factory\n","\n","Please use a **free** Tesla T4 Colab GPU to run this!\n","\n","Project homepage: https://github.com/hiyouga/LLaMA-Factory"]},{"cell_type":"markdown","metadata":{"id":"55MFt9F12aIx"},"source":["### Configuration"]},{"cell_type":"markdown","source":[],"metadata":{"id":"IIuQljeJZhP2"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"GLYa6ODY2UvG","executionInfo":{"status":"ok","timestamp":1715695991813,"user_tz":-120,"elapsed":527,"user":{"displayName":"Bertil Muth","userId":"02558266373806387996"}}},"outputs":[],"source":["# The dataset used in finetuning\n","finetuning_data_url = \"https://raw.githubusercontent.com/bertilmuth/hf_to_gguf/main/finetuning_dataset/FinetuningData_ALL_llamafactory_clean.json\"\n","\n","# The model that is finetuned with the dataset\n","hf_base_model_id=\"microsoft/Phi-3-mini-4k-instruct\"\n","\n","# The llamafactory prompt template, dependent on the base model\n","llamafactory_template_name=\"phi\"\n","\n","# Epochs of finetuning\n","epochs = 2\n","\n","# The\n","# IMPORTANT: You need to set a Google Collab secret called HF_WRITE_TOKEN to a write token of Huggingface for this to work!\n","hf_finetuned_model_id = \"bertilmuth/phi\"\n","\n"]},{"cell_type":"markdown","source":["### Mount Google Drive\n"],"metadata":{"id":"YLXUrEeiQtdF"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4WlSmQyDQx5l","executionInfo":{"status":"ok","timestamp":1715696006152,"user_tz":-120,"elapsed":5139,"user":{"displayName":"Bertil Muth","userId":"02558266373806387996"}},"outputId":"f15009b7-fa27-480a-8167-646e5adfcc6d"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"H9RXn_YQnn9f"},"source":["### Check GPU environment"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"ZkN-ktlsnrdU","executionInfo":{"status":"ok","timestamp":1715696010503,"user_tz":-120,"elapsed":598,"user":{"displayName":"Bertil Muth","userId":"02558266373806387996"}}},"outputs":[],"source":["import torch\n","try:\n","  assert torch.cuda.is_available() is True\n","except AssertionError:\n","  print(\"Please set up a GPU before using LLaMA Factory: https://medium.com/mlearning-ai/training-yolov4-on-google-colab-316f8fff99c6\")"]},{"cell_type":"markdown","source":["### Install Dependencies & Setup"],"metadata":{"id":"2i4Z4qw8aAzI"}},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"giM74oK1rRIH","executionInfo":{"status":"ok","timestamp":1715696098459,"user_tz":-120,"elapsed":78223,"user":{"displayName":"Bertil Muth","userId":"02558266373806387996"}},"outputId":"f95b5a46-2e94-4115-87d2-73bdf73c36ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","Cloning into 'LLaMA-Factory'...\n","remote: Enumerating objects: 11296, done.\u001b[K\n","remote: Counting objects: 100% (91/91), done.\u001b[K\n","remote: Compressing objects: 100% (51/51), done.\u001b[K\n","remote: Total 11296 (delta 40), reused 71 (delta 36), pack-reused 11205\u001b[K\n","Receiving objects: 100% (11296/11296), 215.01 MiB | 14.33 MiB/s, done.\n","Resolving deltas: 100% (8279/8279), done.\n","Updating files: 100% (214/214), done.\n","/content/LLaMA-Factory\n","\u001b[0m\u001b[01;34massets\u001b[0m/       docker-compose.yml  \u001b[01;34mexamples\u001b[0m/  pyproject.toml  requirements.txt  \u001b[01;34msrc\u001b[0m/\n","CITATION.cff  Dockerfile          LICENSE    README.md       \u001b[01;34mscripts\u001b[0m/          \u001b[01;34mtests\u001b[0m/\n","\u001b[01;34mdata\u001b[0m/         \u001b[01;34mevaluation\u001b[0m/         Makefile   README_zh.md    setup.py\n","Collecting unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git\n","  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-b_evyok7/unsloth_151940df3e1348609158570ca1648ce1\n","  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-b_evyok7/unsloth_151940df3e1348609158570ca1648ce1\n","  Resolved https://github.com/unslothai/unsloth.git to commit 47ffd39abd02338e8a5f226d0f529347fb7e5f89\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tyro in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.8.4)\n","Requirement already satisfied: transformers>=4.38.2 in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.40.2)\n","Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.19.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.99)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.66.4)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.5)\n","Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.43.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.25.2)\n","Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.20.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.14.0)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.0.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.31.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\n","Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.9.5)\n","Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.2->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.2->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.2->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.4.3)\n","Requirement already satisfied: docstring-parser>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.16)\n","Requirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.11.0)\n","Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (13.7.1)\n","Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.7.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.0.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.16.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\n","Requirement already satisfied: xformers==0.0.25 in /usr/local/lib/python3.10/dist-packages (0.0.25)\n","Processing /content/LLaMA-Factory\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: transformers>=4.37.2 in /usr/local/lib/python3.10/dist-packages (from llmtuner==0.7.1.dev0) (4.40.2)\n","Requirement already satisfied: datasets>=2.14.3 in /usr/local/lib/python3.10/dist-packages (from llmtuner==0.7.1.dev0) (2.19.1)\n","Requirement already satisfied: accelerate>=0.27.2 in /usr/local/lib/python3.10/dist-packages (from llmtuner==0.7.1.dev0) (0.30.1)\n","Requirement already satisfied: peft>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from llmtuner==0.7.1.dev0) (0.10.0)\n","Requirement already satisfied: trl>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from llmtuner==0.7.1.dev0) (0.8.6)\n","Requirement already satisfied: gradio>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from llmtuner==0.7.1.dev0) (4.31.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from llmtuner==0.7.1.dev0) (1.11.4)\n","Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from llmtuner==0.7.1.dev0) (0.8.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from llmtuner==0.7.1.dev0) (0.1.99)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from llmtuner==0.7.1.dev0) (3.20.3)\n","Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (from llmtuner==0.7.1.dev0) (0.29.0)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from llmtuner==0.7.1.dev0) (2.7.1)\n","Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from llmtuner==0.7.1.dev0) (0.111.0)\n","Requirement already satisfied: sse-starlette in /usr/local/lib/python3.10/dist-packages (from llmtuner==0.7.1.dev0) (2.1.0)\n","Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from llmtuner==0.7.1.dev0) (3.7.1)\n","Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (from llmtuner==0.7.1.dev0) (0.6.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from llmtuner==0.7.1.dev0) (24.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from llmtuner==0.7.1.dev0) (6.0.1)\n","Requirement already satisfied: bitsandbytes>=0.39.0 in /usr/local/lib/python3.10/dist-packages (from llmtuner==0.7.1.dev0) (0.43.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.27.2->llmtuner==0.7.1.dev0) (1.25.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.27.2->llmtuner==0.7.1.dev0) (5.9.5)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.27.2->llmtuner==0.7.1.dev0) (2.2.1+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.27.2->llmtuner==0.7.1.dev0) (0.23.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.27.2->llmtuner==0.7.1.dev0) (0.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llmtuner==0.7.1.dev0) (3.14.0)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llmtuner==0.7.1.dev0) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llmtuner==0.7.1.dev0) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llmtuner==0.7.1.dev0) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llmtuner==0.7.1.dev0) (2.0.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llmtuner==0.7.1.dev0) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llmtuner==0.7.1.dev0) (4.66.4)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llmtuner==0.7.1.dev0) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llmtuner==0.7.1.dev0) (0.70.16)\n","Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llmtuner==0.7.1.dev0) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llmtuner==0.7.1.dev0) (3.9.5)\n","Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llmtuner==0.7.1.dev0) (23.2.1)\n","Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llmtuner==0.7.1.dev0) (4.2.2)\n","Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llmtuner==0.7.1.dev0) (0.3.2)\n","Requirement already satisfied: gradio-client==0.16.3 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llmtuner==0.7.1.dev0) (0.16.3)\n","Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llmtuner==0.7.1.dev0) (0.27.0)\n","Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llmtuner==0.7.1.dev0) (6.4.0)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llmtuner==0.7.1.dev0) (3.1.4)\n","Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llmtuner==0.7.1.dev0) (2.1.5)\n","Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llmtuner==0.7.1.dev0) (3.10.3)\n","Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llmtuner==0.7.1.dev0) (9.4.0)\n","Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llmtuner==0.7.1.dev0) (0.25.1)\n","Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llmtuner==0.7.1.dev0) (0.0.9)\n","Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llmtuner==0.7.1.dev0) (0.4.4)\n","Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llmtuner==0.7.1.dev0) (2.10.0)\n","Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llmtuner==0.7.1.dev0) (0.12.0)\n","Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llmtuner==0.7.1.dev0) (0.12.3)\n","Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llmtuner==0.7.1.dev0) (4.11.0)\n","Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llmtuner==0.7.1.dev0) (2.0.7)\n","Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.16.3->gradio>=4.0.0->llmtuner==0.7.1.dev0) (11.0.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llmtuner==0.7.1.dev0) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llmtuner==0.7.1.dev0) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llmtuner==0.7.1.dev0) (4.51.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llmtuner==0.7.1.dev0) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llmtuner==0.7.1.dev0) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llmtuner==0.7.1.dev0) (2.8.2)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->llmtuner==0.7.1.dev0) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic->llmtuner==0.7.1.dev0) (2.18.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->llmtuner==0.7.1.dev0) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->llmtuner==0.7.1.dev0) (0.19.1)\n","Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.10/dist-packages (from trl>=0.8.1->llmtuner==0.7.1.dev0) (0.8.4)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llmtuner==0.7.1.dev0) (8.1.7)\n","Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llmtuner==0.7.1.dev0) (0.14.0)\n","Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->llmtuner==0.7.1.dev0) (0.37.2)\n","Requirement already satisfied: fastapi-cli>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->llmtuner==0.7.1.dev0) (0.0.3)\n","Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->llmtuner==0.7.1.dev0) (5.10.0)\n","Requirement already satisfied: email_validator>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->llmtuner==0.7.1.dev0) (2.1.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->llmtuner==0.7.1.dev0) (1.16.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->llmtuner==0.7.1.dev0) (2.4.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from sse-starlette->llmtuner==0.7.1.dev0) (3.7.1)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0->llmtuner==0.7.1.dev0) (0.4)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0->llmtuner==0.7.1.dev0) (4.19.2)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0->llmtuner==0.7.1.dev0) (0.12.1)\n","Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi->llmtuner==0.7.1.dev0) (2.6.1)\n","Requirement already satisfied: idna>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi->llmtuner==0.7.1.dev0) (3.7)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->llmtuner==0.7.1.dev0) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->llmtuner==0.7.1.dev0) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->llmtuner==0.7.1.dev0) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->llmtuner==0.7.1.dev0) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->llmtuner==0.7.1.dev0) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->llmtuner==0.7.1.dev0) (4.0.3)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio>=4.0.0->llmtuner==0.7.1.dev0) (2024.2.2)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio>=4.0.0->llmtuner==0.7.1.dev0) (1.0.5)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio>=4.0.0->llmtuner==0.7.1.dev0) (1.3.1)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.14.3->llmtuner==0.7.1.dev0) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.14.3->llmtuner==0.7.1.dev0) (2024.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.14.3->llmtuner==0.7.1.dev0) (3.3.2)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->sse-starlette->llmtuner==0.7.1.dev0) (1.2.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.27.2->llmtuner==0.7.1.dev0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.27.2->llmtuner==0.7.1.dev0) (3.3)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.27.2->llmtuner==0.7.1.dev0) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.27.2->llmtuner==0.7.1.dev0) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.27.2->llmtuner==0.7.1.dev0) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.27.2->llmtuner==0.7.1.dev0) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.27.2->llmtuner==0.7.1.dev0) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.27.2->llmtuner==0.7.1.dev0) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.27.2->llmtuner==0.7.1.dev0) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.27.2->llmtuner==0.7.1.dev0) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.27.2->llmtuner==0.7.1.dev0) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.27.2->llmtuner==0.7.1.dev0) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.27.2->llmtuner==0.7.1.dev0) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.27.2->llmtuner==0.7.1.dev0) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate>=0.27.2->llmtuner==0.7.1.dev0) (12.4.127)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llmtuner==0.7.1.dev0) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llmtuner==0.7.1.dev0) (13.7.1)\n","Requirement already satisfied: docstring-parser>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl>=0.8.1->llmtuner==0.7.1.dev0) (0.16)\n","Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl>=0.8.1->llmtuner==0.7.1.dev0) (1.7.1)\n","Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llmtuner==0.7.1.dev0) (0.6.1)\n","Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llmtuner==0.7.1.dev0) (1.0.1)\n","Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llmtuner==0.7.1.dev0) (0.19.0)\n","Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llmtuner==0.7.1.dev0) (0.21.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->llmtuner==0.7.1.dev0) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->llmtuner==0.7.1.dev0) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->llmtuner==0.7.1.dev0) (0.18.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llmtuner==0.7.1.dev0) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llmtuner==0.7.1.dev0) (2.16.1)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate>=0.27.2->llmtuner==0.7.1.dev0) (1.3.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llmtuner==0.7.1.dev0) (0.1.2)\n","Building wheels for collected packages: llmtuner\n","  Building wheel for llmtuner (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for llmtuner: filename=llmtuner-0.7.1.dev0-py3-none-any.whl size=160773 sha256=608e6cf114440f3a538de608a82ef61b6f523cfb6a25995acc0fcfabe9e6697e\n","  Stored in directory: /root/.cache/pip/wheels/de/aa/c5/27b5682c5592b7c0eecc3e208f176dedf6b11a61cf2a910b85\n","Successfully built llmtuner\n","Installing collected packages: llmtuner\n","  Attempting uninstall: llmtuner\n","    Found existing installation: llmtuner 0.7.1.dev0\n","    Uninstalling llmtuner-0.7.1.dev0:\n","      Successfully uninstalled llmtuner-0.7.1.dev0\n","Successfully installed llmtuner-0.7.1.dev0\n","The content has been successfully written to identity.json.\n"]}],"source":["# Set paths where to store the output\n","adapter_name = llamafactory_template_name + \"_lora\"\n","saved_merged_model_path = adapter_name + \"_merged\"\n","\n","# Install dependencies\n","%cd /content/\n","%rm -rf LLaMA-Factory\n","!git clone https://github.com/hiyouga/LLaMA-Factory.git\n","%cd LLaMA-Factory\n","%ls\n","!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n","!pip install --no-deps xformers==0.0.25\n","!pip install .[bitsandbytes]\n","\n","import os, requests\n","\n","# Download the finetuning data using requests\n","response = requests.get(finetuning_data_url)\n","\n","# Check if the request was successful\n","if response.status_code == 200:\n","    # Extract the text content from the response\n","    text_content = response.text\n","\n","    # Write the text content to the file identity.json\n","    with open(\"/content/LLaMA-Factory/data/identity.json\", \"w\", encoding=\"utf-8\") as file:\n","        file.write(text_content)\n","    print(\"The content has been successfully written to identity.json.\")\n","else:\n","    print(f\"Error: Failed to retrieve the file from {finetuning_data_url}. Status code: {response.status_code}\")\n"]},{"cell_type":"markdown","metadata":{"id":"rgR3UFhB0Ifq"},"source":["### Fine-tune model via Command Line\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6651427,"status":"ok","timestamp":1715702753399,"user":{"displayName":"Bertil Muth","userId":"02558266373806387996"},"user_tz":-120},"id":"CS0Qk5OR0i4Q","outputId":"2560b791-5531-4271-95f0-8546677e5200"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/LLaMA-Factory\n","2024-05-14 14:15:09.331251: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-14 14:15:09.331333: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-14 14:15:09.333492: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-05-14 14:15:11.997693: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","05/14/2024 14:15:16 - WARNING - llmtuner.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n","05/14/2024 14:15:16 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","[INFO|tokenization_utils_base.py:2087] 2024-05-14 14:15:16,938 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/920b6cf52a79ecff578cc33f61922b23cbc88115/tokenizer.model\n","[INFO|tokenization_utils_base.py:2087] 2024-05-14 14:15:16,938 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/920b6cf52a79ecff578cc33f61922b23cbc88115/tokenizer.json\n","[INFO|tokenization_utils_base.py:2087] 2024-05-14 14:15:16,938 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/920b6cf52a79ecff578cc33f61922b23cbc88115/added_tokens.json\n","[INFO|tokenization_utils_base.py:2087] 2024-05-14 14:15:16,938 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/920b6cf52a79ecff578cc33f61922b23cbc88115/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2087] 2024-05-14 14:15:16,938 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/920b6cf52a79ecff578cc33f61922b23cbc88115/tokenizer_config.json\n","[WARNING|logging.py:314] 2024-05-14 14:15:17,012 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","05/14/2024 14:15:17 - INFO - llmtuner.data.template - Replace eos token: <|end|>\n","05/14/2024 14:15:17 - WARNING - llmtuner.data.template - New tokens have been added, make sure `resize_vocab` is True.\n","05/14/2024 14:15:17 - INFO - llmtuner.data.loader - Loading dataset identity.json...\n","Generating train split: 2180 examples [00:00, 20538.52 examples/s]\n","Converting format of dataset: 100% 2180/2180 [00:00<00:00, 44683.93 examples/s]\n","Running tokenizer on dataset: 100% 2180/2180 [00:07<00:00, 304.57 examples/s]\n","input_ids:\n","[1, 32006, 887, 526, 263, 8444, 319, 29902, 20255, 29889, 32007, 29871, 13, 32010, 6204, 263, 1904, 3577, 525, 12636, 442, 6710, 29915, 426, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 5215, 16117, 26841, 6678, 29879, 1057, 275, 29928, 3864, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12715, 822, 9206, 2951, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12715, 822, 9206, 6880, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12715, 822, 853, 908, 11357, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12715, 822, 18199, 11357, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12715, 822, 512, 11506, 5594, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12715, 822, 2796, 5594, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12715, 822, 7370, 2052, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12715, 822, 23186, 2052, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 1595, 9206, 27107, 29912, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 3859, 9206, 2792, 29912, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 3859, 5947, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 3859, 1551, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 8269, 3158, 2069, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 20543, 2069, 769, 5947, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 20543, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 12, 4102, 5947, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 12, 16044, 9206, 2951, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 12, 6098, 1551, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 20543, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 12, 4102, 1551, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 12, 16044, 9206, 6880, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 12, 6098, 5947, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 29913, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 29913, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 1595, 22666, 29912, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 3859, 22666, 2792, 29912, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 3859, 18199, 287, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 3859, 853, 29113, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 8269, 3158, 2069, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 20543, 2069, 769, 18199, 287, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 20543, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 12, 4102, 18199, 287, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 12, 16044, 853, 908, 11357, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 12, 6098, 853, 29113, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 20543, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 12, 4102, 853, 29113, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 12, 16044, 18199, 11357, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 12, 6098, 18199, 287, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 29913, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 29913, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 1595, 8251, 3260, 29912, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 3859, 8251, 2792, 29912, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 3859, 5163, 280, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 3859, 512, 5594, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 8269, 3158, 2069, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 20543, 2069, 769, 5163, 280, 29936, 12]\n","inputs:\n","<s><|system|> You are a helpful AI assistant.<|end|> \n","<|user|> Create a model package 'Smartphone' {\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\timport OccurrenceFunctions::isDuring;\t\t\t\t\t\t\t\t\t\t\n","\tattribute def PowerOn;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\tattribute def PowerOff;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\tattribute def UnlockScreen;\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\tattribute def LockScreen;\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\tattribute def IncomingCall;\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\tattribute def EndCall;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\tattribute def StartApp;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\tattribute def CloseApp;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\tpart PowerManagement{\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\tstate PowerState{\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\tstate Off;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\tstate On;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\tentry action init;\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\ttransition init then Off;\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\ttransition\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\tfirst Off\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\taccept PowerOn\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\tthen On;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\ttransition\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\tfirst On\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\taccept PowerOff\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\tthen Off;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t}\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t}\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\tpart Screen{\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\tstate ScreenState{\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\tstate Locked;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\tstate Unlocked;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\tentry action init;\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\ttransition init then Locked;\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\ttransition\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\tfirst Locked\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\taccept UnlockScreen\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\tthen Unlocked;\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\ttransition\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\tfirst Unlocked\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\taccept LockScreen\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\tthen Locked;\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t}\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t}\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\tpart CallManager{\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\tstate CallState{\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\tstate Idle;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\tstate InCall;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\tentry action init;\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\ttransition init then Idle;\t\n","label_ids:\n","[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 3577, 525, 12636, 442, 6710, 29915, 426, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 5215, 16117, 26841, 6678, 29879, 1057, 275, 29928, 3864, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12715, 822, 9206, 2951, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12715, 822, 9206, 6880, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12715, 822, 853, 908, 11357, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12715, 822, 18199, 11357, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12715, 822, 512, 11506, 5594, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12715, 822, 2796, 5594, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12715, 822, 7370, 2052, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12715, 822, 23186, 2052, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 1595, 9206, 27107, 29912, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 3859, 9206, 2792, 29912, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 3859, 5947, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 3859, 1551, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 8269, 3158, 2069, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 20543, 2069, 769, 5947, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 20543, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 12, 4102, 5947, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 12, 16044, 9206, 2951, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 12, 6098, 1551, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 20543, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 12, 4102, 1551, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 12, 16044, 9206, 6880, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 12, 6098, 5947, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 29913, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 29913, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 1595, 22666, 29912, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 3859, 22666, 2792, 29912, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 3859, 18199, 287, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 3859, 853, 29113, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 8269, 3158, 2069, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 20543, 2069, 769, 18199, 287, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 20543, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 12, 4102, 18199, 287, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 12, 16044, 853, 908, 11357, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 12, 6098, 853, 29113, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 20543, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 12, 4102, 853, 29113, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 12, 16044, 18199, 11357, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 12, 6098, 18199, 287, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 29913, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 29913, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 1595, 8251, 3260, 29912, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 3859, 8251, 2792, 29912, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 3859, 5163, 280, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 3859, 512, 5594, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 8269, 3158, 2069, 29936, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12, 20543, 2069, 769, 5163, 280, 29936, 12]\n","labels:\n","package 'Smartphone' {\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\timport OccurrenceFunctions::isDuring;\t\t\t\t\t\t\t\t\t\t\n","\tattribute def PowerOn;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\tattribute def PowerOff;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\tattribute def UnlockScreen;\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\tattribute def LockScreen;\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\tattribute def IncomingCall;\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\tattribute def EndCall;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\tattribute def StartApp;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\tattribute def CloseApp;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\tpart PowerManagement{\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\tstate PowerState{\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\tstate Off;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\tstate On;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\tentry action init;\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\ttransition init then Off;\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\ttransition\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\tfirst Off\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\taccept PowerOn\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\tthen On;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\ttransition\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\tfirst On\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\taccept PowerOff\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\tthen Off;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t}\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t}\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\tpart Screen{\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\tstate ScreenState{\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\tstate Locked;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\tstate Unlocked;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\tentry action init;\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\ttransition init then Locked;\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\ttransition\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\tfirst Locked\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\taccept UnlockScreen\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\tthen Unlocked;\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\ttransition\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\tfirst Unlocked\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\taccept LockScreen\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\t\tthen Locked;\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t}\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t}\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\tpart CallManager{\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\tstate CallState{\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\tstate Idle;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\tstate InCall;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\tentry action init;\t\t\t\t\t\t\t\t\t\t\t\t\t\n","\t\t\ttransition init then Idle;\t\n","[INFO|configuration_utils.py:726] 2024-05-14 14:15:25,363 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/920b6cf52a79ecff578cc33f61922b23cbc88115/config.json\n","[INFO|configuration_utils.py:726] 2024-05-14 14:15:25,892 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/920b6cf52a79ecff578cc33f61922b23cbc88115/config.json\n","[INFO|configuration_utils.py:789] 2024-05-14 14:15:25,893 >> Model config Phi3Config {\n","  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n","  \"architectures\": [\n","    \"Phi3ForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"auto_map\": {\n","    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n","    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n","  },\n","  \"bos_token_id\": 1,\n","  \"embd_pdrop\": 0.0,\n","  \"eos_token_id\": 32000,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 8192,\n","  \"max_position_embeddings\": 4096,\n","  \"model_type\": \"phi3\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 32,\n","  \"original_max_position_embeddings\": 4096,\n","  \"pad_token_id\": 32000,\n","  \"resid_pdrop\": 0.0,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 10000.0,\n","  \"sliding_window\": 2047,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.40.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32064\n","}\n","\n","05/14/2024 14:15:25 - INFO - llmtuner.model.utils.quantization - Quantizing model to 4 bit.\n","[WARNING|logging.py:329] 2024-05-14 14:15:25,982 >> Unsloth: You passed in `microsoft/Phi-3-mini-4k-instruct` and `load_in_4bit = True`.\n","We shall load `unsloth/Phi-3-mini-4k-instruct-bnb-4bit` for 4x faster loading.\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","[INFO|configuration_utils.py:726] 2024-05-14 14:15:26,243 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--Phi-3-mini-4k-instruct-bnb-4bit/snapshots/1d9acd8f0e086359470add802aae229331797163/config.json\n","[INFO|configuration_utils.py:789] 2024-05-14 14:15:26,244 >> Model config MistralConfig {\n","  \"_name_or_path\": \"unsloth/Phi-3-mini-4k-instruct-bnb-4bit\",\n","  \"architectures\": [\n","    \"MistralForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 32000,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 8192,\n","  \"max_position_embeddings\": 4096,\n","  \"model_type\": \"mistral\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 32,\n","  \"pad_token_id\": 32000,\n","  \"quantization_config\": {\n","    \"_load_in_4bit\": true,\n","    \"_load_in_8bit\": false,\n","    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n","    \"bnb_4bit_quant_storage\": \"uint8\",\n","    \"bnb_4bit_quant_type\": \"nf4\",\n","    \"bnb_4bit_use_double_quant\": true,\n","    \"llm_int8_enable_fp32_cpu_offload\": false,\n","    \"llm_int8_has_fp16_weight\": false,\n","    \"llm_int8_skip_modules\": null,\n","    \"llm_int8_threshold\": 6.0,\n","    \"load_in_4bit\": true,\n","    \"load_in_8bit\": false,\n","    \"quant_method\": \"bitsandbytes\"\n","  },\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_theta\": 10000.0,\n","  \"sliding_window\": 2048,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.40.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32064\n","}\n","\n","==((====))==  Unsloth: Fast Mistral patching release 2024.5\n","   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n","O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n","\\        /    Bfloat16 = FALSE. Xformers = 0.0.25. FA = False.\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n","[INFO|configuration_utils.py:726] 2024-05-14 14:15:26,512 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--Phi-3-mini-4k-instruct-bnb-4bit/snapshots/1d9acd8f0e086359470add802aae229331797163/config.json\n","[INFO|configuration_utils.py:789] 2024-05-14 14:15:26,513 >> Model config MistralConfig {\n","  \"_name_or_path\": \"unsloth/Phi-3-mini-4k-instruct-bnb-4bit\",\n","  \"architectures\": [\n","    \"MistralForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 32000,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 8192,\n","  \"max_position_embeddings\": 4096,\n","  \"model_type\": \"mistral\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 32,\n","  \"pad_token_id\": 32000,\n","  \"quantization_config\": {\n","    \"_load_in_4bit\": true,\n","    \"_load_in_8bit\": false,\n","    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n","    \"bnb_4bit_quant_storage\": \"uint8\",\n","    \"bnb_4bit_quant_type\": \"nf4\",\n","    \"bnb_4bit_use_double_quant\": true,\n","    \"llm_int8_enable_fp32_cpu_offload\": false,\n","    \"llm_int8_has_fp16_weight\": false,\n","    \"llm_int8_skip_modules\": null,\n","    \"llm_int8_threshold\": 6.0,\n","    \"load_in_4bit\": true,\n","    \"load_in_8bit\": false,\n","    \"quant_method\": \"bitsandbytes\"\n","  },\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_theta\": 10000.0,\n","  \"sliding_window\": 2048,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.40.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32064\n","}\n","\n","[INFO|configuration_utils.py:726] 2024-05-14 14:15:26,772 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--Phi-3-mini-4k-instruct-bnb-4bit/snapshots/1d9acd8f0e086359470add802aae229331797163/config.json\n","[INFO|configuration_utils.py:789] 2024-05-14 14:15:26,773 >> Model config MistralConfig {\n","  \"_name_or_path\": \"unsloth/Phi-3-mini-4k-instruct-bnb-4bit\",\n","  \"architectures\": [\n","    \"MistralForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 32000,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 8192,\n","  \"max_position_embeddings\": 4096,\n","  \"model_type\": \"mistral\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 32,\n","  \"pad_token_id\": 32000,\n","  \"quantization_config\": {\n","    \"_load_in_4bit\": true,\n","    \"_load_in_8bit\": false,\n","    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n","    \"bnb_4bit_quant_storage\": \"uint8\",\n","    \"bnb_4bit_quant_type\": \"nf4\",\n","    \"bnb_4bit_use_double_quant\": true,\n","    \"llm_int8_enable_fp32_cpu_offload\": false,\n","    \"llm_int8_has_fp16_weight\": false,\n","    \"llm_int8_skip_modules\": null,\n","    \"llm_int8_threshold\": 6.0,\n","    \"load_in_4bit\": true,\n","    \"load_in_8bit\": false,\n","    \"quant_method\": \"bitsandbytes\"\n","  },\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_theta\": 10000.0,\n","  \"sliding_window\": 2048,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"float16\",\n","  \"transformers_version\": \"4.40.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32064\n","}\n","\n","[WARNING|quantization_config.py:282] 2024-05-14 14:15:27,117 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","[INFO|modeling_utils.py:3429] 2024-05-14 14:15:27,118 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--Phi-3-mini-4k-instruct-bnb-4bit/snapshots/1d9acd8f0e086359470add802aae229331797163/model.safetensors\n","[INFO|modeling_utils.py:1494] 2024-05-14 14:15:27,160 >> Instantiating MistralForCausalLM model under default dtype torch.float16.\n","[INFO|configuration_utils.py:928] 2024-05-14 14:15:27,164 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 32000,\n","  \"pad_token_id\": 32000\n","}\n","\n","[INFO|modeling_utils.py:4170] 2024-05-14 14:15:29,788 >> All model checkpoint weights were used when initializing MistralForCausalLM.\n","\n","[INFO|modeling_utils.py:4178] 2024-05-14 14:15:29,788 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at unsloth/Phi-3-mini-4k-instruct-bnb-4bit.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n","[INFO|configuration_utils.py:883] 2024-05-14 14:15:30,042 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--Phi-3-mini-4k-instruct-bnb-4bit/snapshots/1d9acd8f0e086359470add802aae229331797163/generation_config.json\n","[INFO|configuration_utils.py:928] 2024-05-14 14:15:30,042 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 32000,\n","  \"pad_token_id\": 32000\n","}\n","\n","tokenizer_config.json: 100% 3.17k/3.17k [00:00<00:00, 16.6MB/s]\n","tokenizer.model: 100% 500k/500k [00:00<00:00, 81.1MB/s]\n","added_tokens.json: 100% 293/293 [00:00<00:00, 1.94MB/s]\n","special_tokens_map.json: 100% 571/571 [00:00<00:00, 3.65MB/s]\n","tokenizer.json: 100% 1.84M/1.84M [00:00<00:00, 2.00MB/s]\n","[INFO|tokenization_utils_base.py:2087] 2024-05-14 14:15:33,850 >> loading file tokenizer.model from cache at huggingface_tokenizers_cache/models--unsloth--Phi-3-mini-4k-instruct-bnb-4bit/snapshots/1d9acd8f0e086359470add802aae229331797163/tokenizer.model\n","[INFO|tokenization_utils_base.py:2087] 2024-05-14 14:15:33,851 >> loading file added_tokens.json from cache at huggingface_tokenizers_cache/models--unsloth--Phi-3-mini-4k-instruct-bnb-4bit/snapshots/1d9acd8f0e086359470add802aae229331797163/added_tokens.json\n","[INFO|tokenization_utils_base.py:2087] 2024-05-14 14:15:33,851 >> loading file special_tokens_map.json from cache at huggingface_tokenizers_cache/models--unsloth--Phi-3-mini-4k-instruct-bnb-4bit/snapshots/1d9acd8f0e086359470add802aae229331797163/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2087] 2024-05-14 14:15:33,851 >> loading file tokenizer_config.json from cache at huggingface_tokenizers_cache/models--unsloth--Phi-3-mini-4k-instruct-bnb-4bit/snapshots/1d9acd8f0e086359470add802aae229331797163/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2087] 2024-05-14 14:15:33,851 >> loading file tokenizer.json from cache at huggingface_tokenizers_cache/models--unsloth--Phi-3-mini-4k-instruct-bnb-4bit/snapshots/1d9acd8f0e086359470add802aae229331797163/tokenizer.json\n","[WARNING|logging.py:314] 2024-05-14 14:15:33,965 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","[INFO|tokenization_utils_base.py:2087] 2024-05-14 14:15:34,227 >> loading file tokenizer.model from cache at huggingface_tokenizers_cache/models--unsloth--Phi-3-mini-4k-instruct-bnb-4bit/snapshots/1d9acd8f0e086359470add802aae229331797163/tokenizer.model\n","[INFO|tokenization_utils_base.py:2087] 2024-05-14 14:15:34,227 >> loading file tokenizer.json from cache at huggingface_tokenizers_cache/models--unsloth--Phi-3-mini-4k-instruct-bnb-4bit/snapshots/1d9acd8f0e086359470add802aae229331797163/tokenizer.json\n","[INFO|tokenization_utils_base.py:2087] 2024-05-14 14:15:34,227 >> loading file added_tokens.json from cache at huggingface_tokenizers_cache/models--unsloth--Phi-3-mini-4k-instruct-bnb-4bit/snapshots/1d9acd8f0e086359470add802aae229331797163/added_tokens.json\n","[INFO|tokenization_utils_base.py:2087] 2024-05-14 14:15:34,227 >> loading file special_tokens_map.json from cache at huggingface_tokenizers_cache/models--unsloth--Phi-3-mini-4k-instruct-bnb-4bit/snapshots/1d9acd8f0e086359470add802aae229331797163/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2087] 2024-05-14 14:15:34,228 >> loading file tokenizer_config.json from cache at huggingface_tokenizers_cache/models--unsloth--Phi-3-mini-4k-instruct-bnb-4bit/snapshots/1d9acd8f0e086359470add802aae229331797163/tokenizer_config.json\n","[WARNING|logging.py:314] 2024-05-14 14:15:34,329 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","05/14/2024 14:15:35 - INFO - llmtuner.model.utils.checkpointing - Gradient checkpointing enabled.\n","05/14/2024 14:15:35 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n","05/14/2024 14:15:35 - INFO - llmtuner.model.utils.misc - Found linear modules: gate_proj,o_proj,q_proj,up_proj,k_proj,down_proj,v_proj\n","[WARNING|logging.py:329] 2024-05-14 14:15:36,185 >> Unsloth 2024.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","05/14/2024 14:15:36 - INFO - llmtuner.model.loader - trainable params: 14942208 || all params: 3836021760 || trainable%: 0.3895\n","[INFO|trainer.py:626] 2024-05-14 14:15:36,244 >> Using auto half precision backend\n","05/14/2024 14:15:36 - INFO - llmtuner.train.utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.\n","[WARNING|logging.py:329] 2024-05-14 14:15:36,737 >> ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n","   \\\\   /|    Num examples = 2,180 | Num Epochs = 2\n","O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n","\\        /    Total batch size = 8 | Total steps = 544\n"," \"-____-\"     Number of trainable parameters = 14,942,208\n","{'loss': 0.5277, 'grad_norm': 0.1980551779270172, 'learning_rate': 9.090909090909091e-06, 'epoch': 0.04}\n","{'loss': 0.3923, 'grad_norm': 0.16784800589084625, 'learning_rate': 1.8181818181818182e-05, 'epoch': 0.07}\n","{'loss': 0.2281, 'grad_norm': 0.14287708699703217, 'learning_rate': 2.7272727272727273e-05, 'epoch': 0.11}\n","{'loss': 0.1442, 'grad_norm': 0.136575847864151, 'learning_rate': 3.6363636363636364e-05, 'epoch': 0.15}\n","{'loss': 0.1231, 'grad_norm': 0.10849180072546005, 'learning_rate': 4.545454545454546e-05, 'epoch': 0.18}\n","{'loss': 0.1149, 'grad_norm': 0.08680624514818192, 'learning_rate': 4.998710282169255e-05, 'epoch': 0.22}\n","{'loss': 0.1016, 'grad_norm': 0.08079236000776291, 'learning_rate': 4.988400522336304e-05, 'epoch': 0.26}\n","{'loss': 0.1043, 'grad_norm': 0.0613369457423687, 'learning_rate': 4.967823541074543e-05, 'epoch': 0.29}\n","{'loss': 0.1019, 'grad_norm': 0.07563937455415726, 'learning_rate': 4.937064239677416e-05, 'epoch': 0.33}\n","{'loss': 0.0964, 'grad_norm': 0.06296387314796448, 'learning_rate': 4.896249532022171e-05, 'epoch': 0.37}\n","{'loss': 0.0931, 'grad_norm': 0.06103185936808586, 'learning_rate': 4.845547820919084e-05, 'epoch': 0.4}\n","{'loss': 0.0931, 'grad_norm': 0.07723359018564224, 'learning_rate': 4.7851683032759834e-05, 'epoch': 0.44}\n","{'loss': 0.0907, 'grad_norm': 0.0854564681649208, 'learning_rate': 4.715360106945015e-05, 'epoch': 0.48}\n","{'loss': 0.0907, 'grad_norm': 0.08833709359169006, 'learning_rate': 4.636411262813023e-05, 'epoch': 0.51}\n","{'loss': 0.0882, 'grad_norm': 0.08443668484687805, 'learning_rate': 4.548647516376755e-05, 'epoch': 0.55}\n","{'loss': 0.0861, 'grad_norm': 0.07338166236877441, 'learning_rate': 4.452430983706351e-05, 'epoch': 0.59}\n","{'loss': 0.087, 'grad_norm': 0.10976817458868027, 'learning_rate': 4.3481586573426705e-05, 'epoch': 0.62}\n","{'loss': 0.0849, 'grad_norm': 0.06479723751544952, 'learning_rate': 4.2362607682931677e-05, 'epoch': 0.66}\n","{'loss': 0.0854, 'grad_norm': 0.07418452203273773, 'learning_rate': 4.11719901088477e-05, 'epoch': 0.7}\n","{'loss': 0.0879, 'grad_norm': 0.07341143488883972, 'learning_rate': 3.991464637798083e-05, 'epoch': 0.73}\n","{'loss': 0.0875, 'grad_norm': 0.07445768266916275, 'learning_rate': 3.8595764331428606e-05, 'epoch': 0.77}\n","{'loss': 0.0821, 'grad_norm': 0.09685841202735901, 'learning_rate': 3.72207857193791e-05, 'epoch': 0.81}\n","{'loss': 0.0835, 'grad_norm': 0.07070058584213257, 'learning_rate': 3.5795383748272795e-05, 'epoch': 0.84}\n","{'loss': 0.0766, 'grad_norm': 0.07214401662349701, 'learning_rate': 3.43254396729684e-05, 'epoch': 0.88}\n","{'loss': 0.0782, 'grad_norm': 0.07422274351119995, 'learning_rate': 3.2817018530494164e-05, 'epoch': 0.92}\n","{'loss': 0.0814, 'grad_norm': 0.06439515203237534, 'learning_rate': 3.1276344115508024e-05, 'epoch': 0.95}\n","{'loss': 0.0796, 'grad_norm': 0.0672551691532135, 'learning_rate': 2.9709773300718513e-05, 'epoch': 0.99}\n","{'loss': 0.0767, 'grad_norm': 0.07540100067853928, 'learning_rate': 2.8123769808221405e-05, 'epoch': 1.03}\n","{'loss': 0.071, 'grad_norm': 0.07113685458898544, 'learning_rate': 2.6524877539972263e-05, 'epoch': 1.06}\n","{'loss': 0.0683, 'grad_norm': 0.0685470774769783, 'learning_rate': 2.4919693577434292e-05, 'epoch': 1.1}\n","{'loss': 0.0688, 'grad_norm': 0.07056832313537598, 'learning_rate': 2.3314840961805802e-05, 'epoch': 1.14}\n","{'loss': 0.0705, 'grad_norm': 0.08664701133966446, 'learning_rate': 2.1716941367137e-05, 'epoch': 1.17}\n","{'loss': 0.0683, 'grad_norm': 0.09410739690065384, 'learning_rate': 2.0132587779087632e-05, 'epoch': 1.21}\n","{'loss': 0.0657, 'grad_norm': 0.08070133626461029, 'learning_rate': 1.8568317292053894e-05, 'epoch': 1.25}\n","{'loss': 0.0681, 'grad_norm': 0.07256775349378586, 'learning_rate': 1.7030584136904447e-05, 'epoch': 1.28}\n","{'loss': 0.0702, 'grad_norm': 0.0906452015042305, 'learning_rate': 1.5525733050614193e-05, 'epoch': 1.32}\n","{'loss': 0.0693, 'grad_norm': 0.07289185374975204, 'learning_rate': 1.4059973097673187e-05, 'epoch': 1.36}\n","{'loss': 0.0668, 'grad_norm': 0.07655250281095505, 'learning_rate': 1.2639352051284831e-05, 'epoch': 1.39}\n","{'loss': 0.0679, 'grad_norm': 0.08046351373195648, 'learning_rate': 1.1269731440057097e-05, 'epoch': 1.43}\n","{'loss': 0.0709, 'grad_norm': 0.08264616876840591, 'learning_rate': 9.956762363144892e-06, 'epoch': 1.47}\n","{'loss': 0.0649, 'grad_norm': 0.05626954883337021, 'learning_rate': 8.705862173631412e-06, 'epoch': 1.5}\n","{'loss': 0.0649, 'grad_norm': 0.06915874779224396, 'learning_rate': 7.522192126353156e-06, 'epoch': 1.54}\n","{'loss': 0.068, 'grad_norm': 0.06435028463602066, 'learning_rate': 6.410636082394772e-06, 'epoch': 1.58}\n","{'loss': 0.0658, 'grad_norm': 0.08162558823823929, 'learning_rate': 5.375780358119484e-06, 'epoch': 1.61}\n","{'loss': 0.0683, 'grad_norm': 0.07598299533128738, 'learning_rate': 4.421894801878723e-06, 'epoch': 1.65}\n","{'loss': 0.0669, 'grad_norm': 0.08886800706386566, 'learning_rate': 3.552915176479071e-06, 'epoch': 1.69}\n","{'loss': 0.0649, 'grad_norm': 0.08658226579427719, 'learning_rate': 2.7724269200969905e-06, 'epoch': 1.72}\n","{'loss': 0.063, 'grad_norm': 0.06499702483415604, 'learning_rate': 2.083650352644359e-06, 'epoch': 1.76}\n","{'loss': 0.0663, 'grad_norm': 0.10095097869634628, 'learning_rate': 1.4894273886239208e-06, 'epoch': 1.8}\n","{'loss': 0.0691, 'grad_norm': 0.0680493488907814, 'learning_rate': 9.92209811297612e-07, 'epoch': 1.83}\n","{'loss': 0.0658, 'grad_norm': 0.06788352876901627, 'learning_rate': 5.940491565491813e-07, 'epoch': 1.87}\n","{'loss': 0.0653, 'grad_norm': 0.07385533303022385, 'learning_rate': 2.965882481804433e-07, 'epoch': 1.91}\n","{'loss': 0.0619, 'grad_norm': 0.055467624217271805, 'learning_rate': 1.0105441956688167e-07, 'epoch': 1.94}\n","{'loss': 0.0619, 'grad_norm': 0.07790736109018326, 'learning_rate': 8.254449640257567e-09, 'epoch': 1.98}\n","100% 544/544 [1:50:12<00:00, 12.22s/it][INFO|<string>:474] 2024-05-14 16:05:49,428 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 6612.6907, 'train_samples_per_second': 0.659, 'train_steps_per_second': 0.082, 'train_loss': 0.09621876338496804, 'epoch': 2.0}\n","100% 544/544 [1:50:12<00:00, 12.16s/it]\n","[INFO|trainer.py:3305] 2024-05-14 16:05:49,431 >> Saving model checkpoint to phi_lora\n","[INFO|configuration_utils.py:726] 2024-05-14 16:05:50,133 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--Phi-3-mini-4k-instruct-bnb-4bit/snapshots/1d9acd8f0e086359470add802aae229331797163/config.json\n","[INFO|configuration_utils.py:789] 2024-05-14 16:05:50,134 >> Model config MistralConfig {\n","  \"_name_or_path\": \"unsloth/Phi-3-mini-4k-instruct\",\n","  \"architectures\": [\n","    \"MistralForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 32000,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 8192,\n","  \"max_position_embeddings\": 4096,\n","  \"model_type\": \"mistral\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 32,\n","  \"pad_token_id\": 32000,\n","  \"quantization_config\": {\n","    \"_load_in_4bit\": true,\n","    \"_load_in_8bit\": false,\n","    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n","    \"bnb_4bit_quant_storage\": \"uint8\",\n","    \"bnb_4bit_quant_type\": \"nf4\",\n","    \"bnb_4bit_use_double_quant\": true,\n","    \"llm_int8_enable_fp32_cpu_offload\": false,\n","    \"llm_int8_has_fp16_weight\": false,\n","    \"llm_int8_skip_modules\": null,\n","    \"llm_int8_threshold\": 6.0,\n","    \"load_in_4bit\": true,\n","    \"load_in_8bit\": false,\n","    \"quant_method\": \"bitsandbytes\"\n","  },\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_theta\": 10000.0,\n","  \"sliding_window\": 2048,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.40.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32064\n","}\n","\n","[INFO|tokenization_utils_base.py:2488] 2024-05-14 16:05:50,368 >> tokenizer config file saved in phi_lora/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2497] 2024-05-14 16:05:50,368 >> Special tokens file saved in phi_lora/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =     1.9963\n","  total_flos               = 92966773GF\n","  train_loss               =     0.0962\n","  train_runtime            = 1:50:12.69\n","  train_samples_per_second =      0.659\n","  train_steps_per_second   =      0.082\n","[INFO|modelcard.py:450] 2024-05-14 16:05:50,427 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"]}],"source":["import json\n","\n","args = dict(\n","  stage=\"sft\",                        # do supervised fine-tuning\n","  do_train=True,\n","  model_name_or_path=hf_base_model_id, # model name specified in Configuration\n","  dataset=\"identity\",             # use identity dataset\n","  template=llamafactory_template_name,       # prompt template specified in Configuration\n","  finetuning_type=\"lora\",                   # use LoRA adapters to save memory\n","  lora_target=\"all\",                     # attach LoRA adapters to all linear layers\n","  output_dir=adapter_name,                  # the path to save LoRA adapters\n","  per_device_train_batch_size=2,               # the batch size\n","  gradient_accumulation_steps=4,               # the gradient accumulation steps\n","  lr_scheduler_type=\"cosine\",                 # use cosine learning rate scheduler\n","  logging_steps=10,                      # log every 10 steps\n","  warmup_ratio=0.1,                      # use warmup scheduler\n","  save_steps=1000,                      # save checkpoint every 1000 steps\n","  learning_rate=5e-5,                     # the learning rate\n","  num_train_epochs=epochs,                    # the epochs of training\n","  max_samples=2500,                      # use 500 examples in each dataset\n","  max_grad_norm=1.0,                     # clip gradient norm to 1.0\n","  quantization_bit=4,                     # use 4-bit QLoRA\n","  loraplus_lr_ratio=16.0,                   # use LoRA+ algorithm with lambda=16.0\n","  use_unsloth=False,                      # use UnslothAI's LoRA optimization for 2x faster training\n","  fp16=False,                         # use float16 mixed precision training\n","  overwrite_output_dir=True\n",")\n","\n","json.dump(args, open(\"train.json\", \"w\", encoding=\"utf-8\"), indent=2)\n","\n","%cd /content/LLaMA-Factory/\n","\n","!llamafactory-cli train train.json"]},{"cell_type":"markdown","metadata":{"id":"kTESHaFvbNTr"},"source":["### Login Huggingface"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3069,"status":"ok","timestamp":1715702960818,"user":{"displayName":"Bertil Muth","userId":"02558266373806387996"},"user_tz":-120},"id":"mcNcHcA4bf4Z","outputId":"236f6eb1-0f43-40b1-904e-50b413cfca32"},"outputs":[{"output_type":"stream","name":"stdout","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["# IMPORTANT: You need to set HF_WRITE_TOKEN to a write token of Huggingface for this to work!\n","from google.colab import userdata\n","from huggingface_hub import login\n","\n","login(token=userdata.get('HF_WRITE_TOKEN'))\n"]},{"cell_type":"markdown","source":["### Upload finetuned model to Huggingface"],"metadata":{"id":"psBQveagZ2jN"}},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":880565,"status":"ok","timestamp":1715704663853,"user":{"displayName":"Bertil Muth","userId":"02558266373806387996"},"user_tz":-120},"id":"IMojogHbaOZF","outputId":"96d2517f-2700-4512-fceb-3e4c399228d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/LLaMA-Factory\n","2024-05-14 16:10:13.582230: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-14 16:10:13.582276: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-14 16:10:13.583836: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-05-14 16:10:14.847802: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","[INFO|tokenization_utils_base.py:2087] 2024-05-14 16:10:18,853 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/920b6cf52a79ecff578cc33f61922b23cbc88115/tokenizer.model\n","[INFO|tokenization_utils_base.py:2087] 2024-05-14 16:10:18,853 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/920b6cf52a79ecff578cc33f61922b23cbc88115/tokenizer.json\n","[INFO|tokenization_utils_base.py:2087] 2024-05-14 16:10:18,853 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/920b6cf52a79ecff578cc33f61922b23cbc88115/added_tokens.json\n","[INFO|tokenization_utils_base.py:2087] 2024-05-14 16:10:18,853 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/920b6cf52a79ecff578cc33f61922b23cbc88115/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2087] 2024-05-14 16:10:18,853 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/920b6cf52a79ecff578cc33f61922b23cbc88115/tokenizer_config.json\n","[WARNING|logging.py:314] 2024-05-14 16:10:18,960 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","05/14/2024 16:10:18 - INFO - llmtuner.data.template - Replace eos token: <|end|>\n","05/14/2024 16:10:18 - WARNING - llmtuner.data.template - New tokens have been added, make sure `resize_vocab` is True.\n","[INFO|configuration_utils.py:726] 2024-05-14 16:10:19,220 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/920b6cf52a79ecff578cc33f61922b23cbc88115/config.json\n","[INFO|configuration_utils.py:726] 2024-05-14 16:10:19,745 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/920b6cf52a79ecff578cc33f61922b23cbc88115/config.json\n","[INFO|configuration_utils.py:789] 2024-05-14 16:10:19,746 >> Model config Phi3Config {\n","  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n","  \"architectures\": [\n","    \"Phi3ForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"auto_map\": {\n","    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n","    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n","  },\n","  \"bos_token_id\": 1,\n","  \"embd_pdrop\": 0.0,\n","  \"eos_token_id\": 32000,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 8192,\n","  \"max_position_embeddings\": 4096,\n","  \"model_type\": \"phi3\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 32,\n","  \"original_max_position_embeddings\": 4096,\n","  \"pad_token_id\": 32000,\n","  \"resid_pdrop\": 0.0,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 10000.0,\n","  \"sliding_window\": 2047,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.40.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32064\n","}\n","\n","05/14/2024 16:10:19 - INFO - llmtuner.model.patcher - Using KV cache for faster generation.\n","modeling_phi3.py: 100% 73.8k/73.8k [00:00<00:00, 120MB/s]\n","`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n","Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n","model.safetensors.index.json: 100% 16.3k/16.3k [00:00<00:00, 56.3MB/s]\n","[INFO|modeling_utils.py:3429] 2024-05-14 16:10:21,451 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/920b6cf52a79ecff578cc33f61922b23cbc88115/model.safetensors.index.json\n","Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n","model-00001-of-00002.safetensors:   0% 0.00/4.97G [00:00<?, ?B/s]\u001b[A\n","model-00001-of-00002.safetensors:   0% 21.0M/4.97G [00:00<00:29, 165MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   1% 52.4M/4.97G [00:00<00:22, 216MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   2% 83.9M/4.97G [00:00<00:19, 254MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   3% 126M/4.97G [00:00<00:16, 297MB/s] \u001b[A\n","model-00001-of-00002.safetensors:   3% 157M/4.97G [00:00<00:16, 294MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   4% 189M/4.97G [00:00<00:16, 290MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   4% 220M/4.97G [00:00<00:16, 287MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   5% 252M/4.97G [00:03<01:54, 41.1MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   6% 283M/4.97G [00:03<01:23, 56.0MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   6% 315M/4.97G [00:03<01:06, 69.6MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   7% 336M/4.97G [00:03<01:00, 76.6MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   7% 367M/4.97G [00:03<00:46, 99.2MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   8% 398M/4.97G [00:03<00:37, 121MB/s] \u001b[A\n","model-00001-of-00002.safetensors:   8% 419M/4.97G [00:03<00:36, 126MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   9% 451M/4.97G [00:04<00:29, 154MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  10% 482M/4.97G [00:04<00:25, 177MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  10% 514M/4.97G [00:04<00:22, 202MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  11% 545M/4.97G [00:04<00:19, 227MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  12% 577M/4.97G [00:04<00:17, 247MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  12% 608M/4.97G [00:04<00:16, 257MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  13% 640M/4.97G [00:04<00:18, 236MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  13% 671M/4.97G [00:04<00:17, 242MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  14% 703M/4.97G [00:04<00:17, 239MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  15% 734M/4.97G [00:05<00:18, 233MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  15% 765M/4.97G [00:05<00:22, 189MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  16% 797M/4.97G [00:05<00:19, 210MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  17% 828M/4.97G [00:08<01:55, 35.8MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  17% 849M/4.97G [00:08<01:38, 41.9MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  18% 870M/4.97G [00:08<01:22, 49.8MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  18% 902M/4.97G [00:08<00:59, 68.8MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  19% 933M/4.97G [00:08<00:44, 90.0MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  19% 965M/4.97G [00:08<00:35, 112MB/s] \u001b[A\n","model-00001-of-00002.safetensors:  20% 986M/4.97G [00:08<00:33, 118MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  21% 1.03G/4.97G [00:09<00:24, 161MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  21% 1.06G/4.97G [00:09<00:22, 176MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  22% 1.09G/4.97G [00:09<00:24, 157MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  22% 1.11G/4.97G [00:09<00:23, 162MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  23% 1.14G/4.97G [00:09<00:21, 182MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  24% 1.17G/4.97G [00:09<00:18, 206MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  24% 1.21G/4.97G [00:09<00:17, 209MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  25% 1.24G/4.97G [00:10<00:19, 188MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  25% 1.26G/4.97G [00:10<00:28, 130MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  26% 1.28G/4.97G [00:10<00:36, 101MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  26% 1.30G/4.97G [00:11<00:34, 107MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  27% 1.32G/4.97G [00:11<00:36, 101MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  27% 1.34G/4.97G [00:11<00:40, 89.3MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  27% 1.36G/4.97G [00:11<00:39, 91.8MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  28% 1.37G/4.97G [00:11<00:45, 79.4MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  28% 1.38G/4.97G [00:12<00:56, 63.4MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  28% 1.39G/4.97G [00:12<00:52, 68.1MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  28% 1.42G/4.97G [00:12<00:43, 80.8MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  29% 1.44G/4.97G [00:12<00:36, 97.7MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  30% 1.47G/4.97G [00:12<00:25, 137MB/s] \u001b[A\n","model-00001-of-00002.safetensors:  30% 1.49G/4.97G [00:12<00:23, 147MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  31% 1.52G/4.97G [00:13<00:19, 173MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  31% 1.55G/4.97G [00:13<00:17, 192MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  32% 1.57G/4.97G [00:13<00:19, 178MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  32% 1.60G/4.97G [00:13<00:16, 207MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  33% 1.64G/4.97G [00:13<00:25, 131MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  33% 1.66G/4.97G [00:13<00:23, 142MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  34% 1.68G/4.97G [00:14<00:26, 123MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  34% 1.71G/4.97G [00:14<00:21, 149MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  35% 1.74G/4.97G [00:14<00:18, 172MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  36% 1.77G/4.97G [00:14<00:16, 195MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  36% 1.80G/4.97G [00:14<00:18, 173MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  37% 1.82G/4.97G [00:14<00:17, 176MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  37% 1.85G/4.97G [00:15<00:17, 180MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  38% 1.87G/4.97G [00:16<01:28, 35.3MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  38% 1.90G/4.97G [00:17<00:59, 51.5MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  39% 1.93G/4.97G [00:17<00:42, 71.7MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  39% 1.96G/4.97G [00:17<00:32, 93.4MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  40% 1.99G/4.97G [00:17<00:25, 119MB/s] \u001b[A\n","model-00001-of-00002.safetensors:  41% 2.02G/4.97G [00:17<00:20, 145MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  41% 2.06G/4.97G [00:17<00:17, 170MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  42% 2.09G/4.97G [00:17<00:15, 186MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  43% 2.12G/4.97G [00:17<00:14, 201MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  43% 2.15G/4.97G [00:18<00:13, 213MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  44% 2.18G/4.97G [00:18<00:12, 224MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  44% 2.21G/4.97G [00:18<00:12, 226MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  45% 2.24G/4.97G [00:18<00:11, 243MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  46% 2.28G/4.97G [00:18<00:11, 239MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  46% 2.31G/4.97G [00:18<00:11, 241MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  47% 2.34G/4.97G [00:18<00:10, 247MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  48% 2.37G/4.97G [00:18<00:10, 255MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  48% 2.40G/4.97G [00:19<00:13, 193MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  49% 2.43G/4.97G [00:19<00:14, 170MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  50% 2.46G/4.97G [00:19<00:12, 195MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  50% 2.50G/4.97G [00:19<00:11, 210MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  51% 2.53G/4.97G [00:19<00:11, 207MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  51% 2.56G/4.97G [00:19<00:11, 203MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  52% 2.59G/4.97G [00:20<00:11, 206MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  53% 2.62G/4.97G [00:24<01:37, 24.2MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  53% 2.64G/4.97G [00:24<01:17, 30.0MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  54% 2.66G/4.97G [00:24<01:06, 34.7MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  54% 2.68G/4.97G [00:24<01:00, 37.8MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  54% 2.71G/4.97G [00:24<00:47, 48.2MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  55% 2.74G/4.97G [00:25<00:32, 68.0MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  56% 2.77G/4.97G [00:25<00:25, 88.1MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  56% 2.79G/4.97G [00:25<00:21, 103MB/s] \u001b[A\n","model-00001-of-00002.safetensors:  57% 2.81G/4.97G [00:25<00:19, 113MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  57% 2.83G/4.97G [00:25<00:16, 126MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  57% 2.85G/4.97G [00:25<00:15, 136MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  58% 2.87G/4.97G [00:25<00:14, 143MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  58% 2.90G/4.97G [00:26<00:12, 169MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  59% 2.94G/4.97G [00:26<00:11, 180MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  59% 2.96G/4.97G [00:26<00:10, 186MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  60% 2.99G/4.97G [00:26<00:10, 198MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  61% 3.02G/4.97G [00:26<00:09, 207MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  61% 3.05G/4.97G [00:28<00:52, 36.5MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  62% 3.07G/4.97G [00:29<00:42, 44.5MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  62% 3.09G/4.97G [00:29<00:36, 51.9MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  63% 3.12G/4.97G [00:29<00:26, 70.7MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  63% 3.16G/4.97G [00:29<00:19, 94.7MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  64% 3.19G/4.97G [00:29<00:14, 119MB/s] \u001b[A\n","model-00001-of-00002.safetensors:  65% 3.22G/4.97G [00:29<00:13, 128MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  65% 3.24G/4.97G [00:29<00:12, 134MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  66% 3.27G/4.97G [00:30<00:10, 165MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  66% 3.30G/4.97G [00:33<01:12, 22.9MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  67% 3.32G/4.97G [00:34<01:01, 26.8MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  68% 3.37G/4.97G [00:34<00:37, 42.8MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  68% 3.40G/4.97G [00:34<00:27, 57.2MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  69% 3.43G/4.97G [00:34<00:20, 73.7MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  70% 3.46G/4.97G [00:34<00:16, 89.8MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  70% 3.49G/4.97G [00:34<00:13, 111MB/s] \u001b[A\n","model-00001-of-00002.safetensors:  71% 3.52G/4.97G [00:35<00:10, 133MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  71% 3.55G/4.97G [00:35<00:13, 104MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  72% 3.58G/4.97G [00:35<00:11, 117MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  72% 3.60G/4.97G [00:35<00:11, 118MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  73% 3.63G/4.97G [00:35<00:09, 146MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  74% 3.66G/4.97G [00:36<00:08, 164MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  74% 3.69G/4.97G [00:36<00:06, 184MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  75% 3.72G/4.97G [00:36<00:07, 171MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  75% 3.74G/4.97G [00:36<00:06, 178MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  76% 3.76G/4.97G [00:38<00:29, 41.4MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  76% 3.79G/4.97G [00:38<00:22, 52.3MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  77% 3.82G/4.97G [00:38<00:15, 73.3MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  77% 3.85G/4.97G [00:38<00:11, 95.5MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  78% 3.88G/4.97G [00:38<00:09, 119MB/s] \u001b[A\n","model-00001-of-00002.safetensors:  79% 3.91G/4.97G [00:38<00:07, 142MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  79% 3.94G/4.97G [00:39<00:07, 138MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  80% 3.97G/4.97G [00:39<00:06, 160MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  81% 4.01G/4.97G [00:39<00:05, 171MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  81% 4.04G/4.97G [00:39<00:04, 197MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  82% 4.07G/4.97G [00:39<00:04, 219MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  82% 4.10G/4.97G [00:39<00:04, 188MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  83% 4.13G/4.97G [00:39<00:04, 184MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  84% 4.16G/4.97G [00:40<00:04, 193MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  84% 4.19G/4.97G [00:40<00:03, 204MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  85% 4.23G/4.97G [00:40<00:03, 209MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  86% 4.26G/4.97G [00:40<00:03, 216MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  86% 4.29G/4.97G [00:40<00:03, 207MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  87% 4.32G/4.97G [00:40<00:02, 221MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  88% 4.36G/4.97G [00:40<00:02, 247MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  88% 4.39G/4.97G [00:41<00:02, 255MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  89% 4.42G/4.97G [00:41<00:02, 259MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  90% 4.46G/4.97G [00:41<00:02, 198MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  90% 4.49G/4.97G [00:41<00:02, 201MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  91% 4.52G/4.97G [00:43<00:11, 39.4MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  91% 4.54G/4.97G [00:44<00:09, 47.2MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  92% 4.56G/4.97G [00:44<00:07, 54.0MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  92% 4.58G/4.97G [00:44<00:06, 57.9MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  93% 4.62G/4.97G [00:44<00:03, 88.1MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  94% 4.66G/4.97G [00:44<00:02, 110MB/s] \u001b[A\n","model-00001-of-00002.safetensors:  94% 4.68G/4.97G [00:44<00:02, 120MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  95% 4.71G/4.97G [00:45<00:01, 145MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  95% 4.74G/4.97G [00:45<00:01, 171MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  96% 4.77G/4.97G [00:45<00:01, 194MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  97% 4.80G/4.97G [00:45<00:00, 207MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  97% 4.83G/4.97G [00:45<00:00, 227MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  98% 4.87G/4.97G [00:45<00:00, 242MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  98% 4.90G/4.97G [00:45<00:00, 246MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  99% 4.93G/4.97G [00:45<00:00, 251MB/s]\u001b[A\n","model-00001-of-00002.safetensors: 100% 4.97G/4.97G [00:46<00:00, 108MB/s]\n","Downloading shards:  50% 1/2 [00:46<00:46, 46.64s/it]\n","model-00002-of-00002.safetensors:   0% 0.00/2.67G [00:00<?, ?B/s]\u001b[A\n","model-00002-of-00002.safetensors:   1% 31.5M/2.67G [00:00<00:10, 250MB/s]\u001b[A\n","model-00002-of-00002.safetensors:   3% 73.4M/2.67G [00:00<00:09, 287MB/s]\u001b[A\n","model-00002-of-00002.safetensors:   4% 105M/2.67G [00:00<00:09, 266MB/s] \u001b[A\n","model-00002-of-00002.safetensors:   5% 136M/2.67G [00:00<00:10, 236MB/s]\u001b[A\n","model-00002-of-00002.safetensors:   6% 168M/2.67G [00:00<00:11, 217MB/s]\u001b[A\n","model-00002-of-00002.safetensors:   7% 199M/2.67G [00:00<00:10, 229MB/s]\u001b[A\n","model-00002-of-00002.safetensors:   9% 231M/2.67G [00:02<00:44, 55.1MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  10% 262M/2.67G [00:02<00:33, 72.3MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  11% 283M/2.67G [00:02<00:31, 76.5MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  12% 315M/2.67G [00:02<00:24, 97.6MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  13% 346M/2.67G [00:02<00:18, 124MB/s] \u001b[A\n","model-00002-of-00002.safetensors:  14% 377M/2.67G [00:03<00:15, 147MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  15% 409M/2.67G [00:03<00:13, 166MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  16% 440M/2.67G [00:03<00:12, 173MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  18% 472M/2.67G [00:03<00:11, 195MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  19% 503M/2.67G [00:03<00:10, 209MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  20% 535M/2.67G [00:03<00:09, 222MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  21% 566M/2.67G [00:03<00:08, 238MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  22% 598M/2.67G [00:03<00:08, 242MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  24% 629M/2.67G [00:04<00:08, 228MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  25% 661M/2.67G [00:04<00:08, 239MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  26% 692M/2.67G [00:04<00:07, 250MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  27% 724M/2.67G [00:04<00:07, 251MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  28% 755M/2.67G [00:04<00:07, 265MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  29% 786M/2.67G [00:04<00:07, 255MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  31% 818M/2.67G [00:04<00:07, 263MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  32% 849M/2.67G [00:04<00:07, 243MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  33% 881M/2.67G [00:05<00:07, 227MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  34% 912M/2.67G [00:05<00:07, 230MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  35% 944M/2.67G [00:05<00:07, 227MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  37% 975M/2.67G [00:05<00:08, 207MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  38% 1.01G/2.67G [00:05<00:08, 207MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  39% 1.04G/2.67G [00:05<00:07, 217MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  40% 1.07G/2.67G [00:06<00:07, 224MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  41% 1.10G/2.67G [00:06<00:06, 232MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  42% 1.13G/2.67G [00:06<00:06, 236MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  44% 1.16G/2.67G [00:06<00:06, 248MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  45% 1.21G/2.67G [00:06<00:05, 252MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  46% 1.24G/2.67G [00:06<00:07, 196MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  48% 1.27G/2.67G [00:07<00:12, 116MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  48% 1.29G/2.67G [00:07<00:10, 127MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  49% 1.31G/2.67G [00:07<00:09, 140MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  50% 1.33G/2.67G [00:07<00:08, 151MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  51% 1.35G/2.67G [00:07<00:08, 162MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  51% 1.37G/2.67G [00:07<00:07, 171MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  53% 1.41G/2.67G [00:08<00:06, 181MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  53% 1.43G/2.67G [00:08<00:06, 187MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  55% 1.46G/2.67G [00:08<00:05, 206MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  56% 1.49G/2.67G [00:08<00:05, 225MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  57% 1.52G/2.67G [00:08<00:04, 240MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  58% 1.55G/2.67G [00:08<00:04, 235MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  59% 1.58G/2.67G [00:08<00:04, 235MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  60% 1.61G/2.67G [00:08<00:05, 199MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  62% 1.65G/2.67G [00:09<00:05, 202MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  63% 1.68G/2.67G [00:12<00:34, 28.6MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  64% 1.70G/2.67G [00:12<00:28, 33.9MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  65% 1.73G/2.67G [00:12<00:19, 47.1MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  66% 1.76G/2.67G [00:12<00:14, 63.8MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  67% 1.79G/2.67G [00:12<00:10, 82.4MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  68% 1.81G/2.67G [00:13<00:08, 95.1MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  69% 1.84G/2.67G [00:13<00:07, 106MB/s] \u001b[A\n","model-00002-of-00002.safetensors:  70% 1.86G/2.67G [00:13<00:07, 115MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  70% 1.88G/2.67G [00:13<00:07, 103MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  71% 1.90G/2.67G [00:17<00:44, 17.5MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  72% 1.93G/2.67G [00:17<00:27, 26.9MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  73% 1.96G/2.67G [00:17<00:18, 39.1MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  74% 1.98G/2.67G [00:17<00:14, 47.0MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  75% 2.00G/2.67G [00:17<00:11, 57.1MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  76% 2.03G/2.67G [00:18<00:08, 78.9MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  77% 2.07G/2.67G [00:18<00:05, 102MB/s] \u001b[A\n","model-00002-of-00002.safetensors:  78% 2.09G/2.67G [00:18<00:05, 112MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  79% 2.11G/2.67G [00:18<00:04, 125MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  80% 2.14G/2.67G [00:18<00:03, 154MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  81% 2.17G/2.67G [00:18<00:02, 181MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  82% 2.20G/2.67G [00:18<00:02, 201MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  84% 2.23G/2.67G [00:18<00:01, 219MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  85% 2.26G/2.67G [00:19<00:01, 239MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  86% 2.30G/2.67G [00:19<00:01, 248MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  87% 2.33G/2.67G [00:19<00:01, 259MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  88% 2.36G/2.67G [00:19<00:01, 251MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  90% 2.39G/2.67G [00:19<00:01, 259MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  91% 2.42G/2.67G [00:19<00:00, 262MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  92% 2.45G/2.67G [00:19<00:00, 241MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  93% 2.49G/2.67G [00:19<00:00, 223MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  94% 2.52G/2.67G [00:20<00:00, 217MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  95% 2.55G/2.67G [00:20<00:00, 228MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  97% 2.58G/2.67G [00:20<00:00, 227MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  98% 2.61G/2.67G [00:20<00:00, 230MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  99% 2.64G/2.67G [00:20<00:00, 233MB/s]\u001b[A\n","model-00002-of-00002.safetensors: 100% 2.67G/2.67G [00:20<00:00, 129MB/s]\n","Downloading shards: 100% 2/2 [01:07<00:00, 33.95s/it]\n","[INFO|modeling_utils.py:1494] 2024-05-14 16:11:29,361 >> Instantiating Phi3ForCausalLM model under default dtype torch.float16.\n","[INFO|configuration_utils.py:928] 2024-05-14 16:11:29,363 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 32000,\n","  \"pad_token_id\": 32000\n","}\n","\n","Loading checkpoint shards: 100% 2/2 [00:40<00:00, 20.25s/it]\n","[INFO|modeling_utils.py:4170] 2024-05-14 16:12:09,984 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n","\n","[INFO|modeling_utils.py:4178] 2024-05-14 16:12:09,984 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n","generation_config.json: 100% 172/172 [00:00<00:00, 913kB/s]\n","[INFO|configuration_utils.py:883] 2024-05-14 16:12:10,546 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/920b6cf52a79ecff578cc33f61922b23cbc88115/generation_config.json\n","[INFO|configuration_utils.py:928] 2024-05-14 16:12:10,547 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": [\n","    32000,\n","    32001,\n","    32007\n","  ],\n","  \"pad_token_id\": 32000\n","}\n","\n","05/14/2024 16:12:10 - INFO - llmtuner.model.utils.attention - Using vanilla Attention implementation.\n","05/14/2024 16:12:10 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n","05/14/2024 16:12:22 - INFO - llmtuner.model.adapter - Merged 1 adapter(s).\n","05/14/2024 16:12:22 - INFO - llmtuner.model.adapter - Loaded adapter(s): phi_lora\n","05/14/2024 16:12:22 - INFO - llmtuner.model.loader - all params: 3821079552\n","[INFO|configuration_utils.py:471] 2024-05-14 16:12:34,683 >> Configuration saved in phi_lora_merged/config.json\n","[INFO|configuration_utils.py:697] 2024-05-14 16:12:34,683 >> Configuration saved in phi_lora_merged/generation_config.json\n","[INFO|modeling_utils.py:2598] 2024-05-14 16:21:18,974 >> The model is bigger than the maximum size per checkpoint (2GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at phi_lora_merged/model.safetensors.index.json.\n","[INFO|configuration_utils.py:471] 2024-05-14 16:21:19,871 >> Configuration saved in /tmp/tmpz3wavz_8/config.json\n","[INFO|configuration_utils.py:697] 2024-05-14 16:21:19,872 >> Configuration saved in /tmp/tmpz3wavz_8/generation_config.json\n","[INFO|modeling_utils.py:2598] 2024-05-14 16:34:26,268 >> The model is bigger than the maximum size per checkpoint (2GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /tmp/tmpz3wavz_8/model.safetensors.index.json.\n","[INFO|hub.py:757] 2024-05-14 16:35:17,179 >> Uploading the following files to bertilmuth/phi: model-00004-of-00004.safetensors,config.json,model.safetensors.index.json,model-00001-of-00004.safetensors,model-00002-of-00004.safetensors,README.md,model-00003-of-00004.safetensors,configuration_phi3.py,generation_config.json,modeling_phi3.py\n","model-00004-of-00004.safetensors:   0% 0.00/1.76G [00:00<?, ?B/s]\n","model-00002-of-00004.safetensors:   0% 0.00/1.94G [00:00<?, ?B/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:   0% 0.00/1.96G [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","Upload 4 LFS files:   0% 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:   0% 0.00/1.98G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00004-of-00004.safetensors:   0% 16.4k/1.76G [00:00<5:52:59, 83.3kB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:   0% 16.4k/1.98G [00:00<6:39:18, 82.7kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00004-of-00004.safetensors:   0% 1.69M/1.76G [00:00<04:39, 6.31MB/s]  \n","\n","model-00001-of-00004.safetensors:   0% 672k/1.96G [00:00<12:34, 2.60MB/s]   \u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:   0% 1.65M/1.98G [00:00<05:16, 6.26MB/s]  \u001b[A\u001b[A\u001b[A\u001b[A\n","model-00004-of-00004.safetensors:   0% 2.46M/1.76G [00:00<05:57, 4.93MB/s]\n","model-00002-of-00004.safetensors:   0% 2.42M/1.94G [00:00<07:17, 4.42MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:   0% 1.06M/1.96G [00:00<30:08, 1.08MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:   0% 2.44M/1.98G [00:01<15:46, 2.09MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:   0% 4.42M/1.96G [00:01<06:05, 5.34MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:   0% 3.05M/1.76G [00:01<14:59, 1.96MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:   0% 5.14M/1.98G [00:01<06:08, 5.36MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00004-of-00004.safetensors:   0% 5.11M/1.76G [00:01<06:52, 4.26MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:   0% 6.37M/1.98G [00:01<05:18, 6.20MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00004-of-00004.safetensors:   0% 6.47M/1.76G [00:01<05:36, 5.22MB/s]\n","\n","\n","\n","model-00004-of-00004.safetensors:   0% 8.27M/1.76G [00:01<04:07, 7.10MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:   1% 10.4M/1.98G [00:01<02:59, 11.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:   0% 8.37M/1.96G [00:01<04:44, 6.85MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:   1% 13.1M/1.98G [00:01<02:15, 14.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00004-of-00004.safetensors:   1% 10.0M/1.76G [00:01<03:20, 8.76MB/s]\n","model-00004-of-00004.safetensors:   1% 12.6M/1.76G [00:01<02:27, 11.9MB/s]\n","model-00002-of-00004.safetensors:   0% 5.28M/1.94G [00:01<11:03, 2.91MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:   1% 10.6M/1.96G [00:02<06:03, 5.36MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:   0% 6.44M/1.94G [00:02<08:48, 3.65MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   0% 8.19M/1.94G [00:02<06:07, 5.25MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:   1% 11.5M/1.96G [00:02<06:18, 5.14MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:   1% 9.93M/1.94G [00:02<04:39, 6.89MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:   1% 12.6M/1.96G [00:02<05:29, 5.91MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:   1% 16.0M/1.98G [00:02<04:27, 7.36MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:   1% 12.4M/1.94G [00:02<03:12, 9.99MB/s]\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:   1% 16.0M/1.76G [00:02<04:01, 7.22MB/s]\n","\n","model-00004-of-00004.safetensors:   1% 21.0M/1.76G [00:02<02:21, 12.3MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:   1% 22.7M/1.98G [00:02<02:32, 12.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:   1% 24.2M/1.76G [00:02<02:05, 13.8MB/s]\n","model-00002-of-00004.safetensors:   1% 16.0M/1.94G [00:03<05:21, 5.98MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   1% 20.8M/1.94G [00:03<03:08, 10.2MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:   1% 16.1M/1.96G [00:03<09:14, 3.50MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:   1% 23.1M/1.94G [00:03<02:46, 11.5MB/s]\u001b[A\n","\n","model-00004-of-00004.safetensors:   2% 27.5M/1.76G [00:03<03:37, 7.99MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:   1% 27.9M/1.98G [00:03<04:50, 6.73MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:   1% 26.4M/1.94G [00:03<02:26, 13.1MB/s]\u001b[A\n","\n","model-00004-of-00004.safetensors:   2% 29.2M/1.76G [00:03<03:38, 7.95MB/s]\n","\n","\n","\n","model-00004-of-00004.safetensors:   2% 30.6M/1.76G [00:03<03:23, 8.51MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:   2% 31.6M/1.98G [00:03<03:54, 8.33MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:   1% 26.8M/1.96G [00:04<04:40, 6.90MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:   1% 28.4M/1.94G [00:04<04:40, 6.80MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:   1% 28.3M/1.96G [00:04<04:57, 6.50MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:   2% 30.0M/1.94G [00:04<04:24, 7.22MB/s]\u001b[A\n","model-00004-of-00004.safetensors:   2% 32.0M/1.76G [00:04<06:24, 4.50MB/s]\n","\n","model-00001-of-00004.safetensors:   2% 29.8M/1.96G [00:04<04:42, 6.84MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:   2% 40.5M/1.76G [00:04<02:31, 11.4MB/s]\n","\n","model-00001-of-00004.safetensors:   2% 31.6M/1.96G [00:04<04:01, 7.97MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:   2% 38.9M/1.98G [00:05<03:50, 8.43MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:   4% 63.4M/1.76G [00:05<01:05, 26.0MB/s]\n","model-00002-of-00004.safetensors:   2% 33.0M/1.94G [00:05<07:33, 4.20MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   2% 38.2M/1.94G [00:05<03:49, 8.26MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:   2% 32.8M/1.96G [00:05<07:51, 4.08MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:   2% 37.6M/1.96G [00:05<03:55, 8.15MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:   2% 40.5M/1.94G [00:05<03:22, 9.36MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:   2% 44.0M/1.98G [00:06<05:15, 6.15MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00004-of-00004.safetensors:   4% 69.6M/1.76G [00:06<01:26, 19.5MB/s]\n","\n","\n","\n","model-00004-of-00004.safetensors:   4% 74.2M/1.76G [00:06<01:18, 21.5MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:   2% 47.6M/1.98G [00:06<04:07, 7.80MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:   2% 44.0M/1.94G [00:06<05:08, 6.14MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:   2% 44.0M/1.96G [00:07<05:05, 6.26MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:   2% 45.9M/1.94G [00:07<04:36, 6.85MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   2% 47.6M/1.94G [00:07<04:02, 7.81MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:   2% 46.0M/1.96G [00:07<04:36, 6.92MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:   2% 49.0M/1.98G [00:07<07:25, 4.34MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:   3% 53.2M/1.98G [00:07<04:18, 7.45MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00004-of-00004.safetensors:   4% 78.5M/1.76G [00:07<02:36, 10.7MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:   3% 56.7M/1.98G [00:07<03:18, 9.71MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:   3% 49.1M/1.94G [00:08<07:14, 4.35MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   3% 54.8M/1.94G [00:08<03:37, 8.66MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:   3% 58.7M/1.98G [00:08<05:04, 6.32MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:   3% 57.2M/1.94G [00:08<03:11, 9.80MB/s]\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:   5% 81.6M/1.76G [00:08<03:48, 7.37MB/s]\n","\n","model-00004-of-00004.safetensors:   5% 87.4M/1.76G [00:08<02:42, 10.3MB/s]\n","\n","model-00001-of-00004.safetensors:   3% 58.3M/1.96G [00:08<03:08, 10.1MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:   3% 61.9M/1.98G [00:08<04:43, 6.76MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:   3% 63.3M/1.98G [00:08<04:16, 7.49MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:   3% 59.4M/1.94G [00:09<04:50, 6.47MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:   3% 64.0M/1.96G [00:09<03:00, 10.5MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:   4% 70.0M/1.96G [00:09<02:07, 14.8MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:   3% 61.1M/1.94G [00:09<04:44, 6.59MB/s]\u001b[A\n","model-00004-of-00004.safetensors:   5% 91.3M/1.76G [00:09<03:40, 7.58MB/s]\n","model-00002-of-00004.safetensors:   3% 63.7M/1.94G [00:09<04:11, 7.44MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:   3% 64.5M/1.98G [00:09<08:25, 3.79MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:   5% 93.7M/1.76G [00:09<03:32, 7.85MB/s]\n","\n","\n","\n","model-00004-of-00004.safetensors:   5% 95.7M/1.76G [00:09<03:10, 8.74MB/s]\n","\n","model-00001-of-00004.safetensors:   4% 74.6M/1.96G [00:10<03:09, 9.94MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:   4% 77.4M/1.96G [00:10<03:02, 10.3MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:   3% 64.9M/1.94G [00:10<08:04, 3.87MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:   4% 79.7M/1.96G [00:10<02:59, 10.5MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:   4% 70.1M/1.94G [00:10<03:44, 8.30MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:   4% 75.1M/1.98G [00:10<04:49, 6.59MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00004-of-00004.safetensors:   6% 97.7M/1.76G [00:10<05:02, 5.52MB/s]\n","\n","\n","\n","model-00004-of-00004.safetensors:   6% 103M/1.76G [00:10<03:06, 8.90MB/s] \n","\n","\n","\n","model-00003-of-00004.safetensors:   4% 78.1M/1.98G [00:10<04:32, 6.98MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:   4% 74.9M/1.94G [00:11<05:01, 6.17MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:   4% 81.7M/1.96G [00:11<05:06, 6.12MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:   4% 86.5M/1.96G [00:11<03:20, 9.35MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:   4% 76.5M/1.94G [00:11<04:58, 6.24MB/s]\u001b[A\n","model-00004-of-00004.safetensors:   6% 107M/1.76G [00:11<04:06, 6.72MB/s]\n","model-00002-of-00004.safetensors:   4% 79.6M/1.94G [00:11<04:02, 7.67MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:   4% 80.0M/1.98G [00:11<07:46, 4.08MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:   6% 111M/1.76G [00:12<03:25, 8.06MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:   4% 89.0M/1.98G [00:12<03:05, 10.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:   5% 90.5M/1.96G [00:12<04:15, 7.31MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:   5% 92.4M/1.96G [00:12<04:18, 7.21MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:   5% 94.1M/1.96G [00:12<04:09, 7.48MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:   4% 80.8M/1.94G [00:12<07:54, 3.91MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   4% 86.8M/1.94G [00:12<03:27, 8.94MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:   5% 91.2M/1.98G [00:12<04:35, 6.87MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00004-of-00004.safetensors:   6% 113M/1.76G [00:13<05:29, 5.01MB/s]\n","\n","\n","\n","model-00004-of-00004.safetensors:   7% 118M/1.76G [00:13<03:18, 8.30MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:   5% 95.3M/1.98G [00:13<03:44, 8.40MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:   5% 96.0M/1.96G [00:13<06:35, 4.71MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:   6% 110M/1.96G [00:13<02:03, 15.0MB/s] \u001b[A\u001b[A\n","model-00002-of-00004.safetensors:   5% 91.3M/1.94G [00:13<05:08, 5.99MB/s]\u001b[A\n","model-00004-of-00004.safetensors:   7% 123M/1.76G [00:14<04:02, 6.76MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:   5% 96.8M/1.98G [00:14<06:49, 4.61MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:   5% 95.3M/1.94G [00:14<03:59, 7.70MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:   6% 110M/1.98G [00:14<02:05, 14.9MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00004-of-00004.safetensors:   7% 125M/1.76G [00:14<03:58, 6.86MB/s]\n","\n","model-00004-of-00004.safetensors:   7% 127M/1.76G [00:14<03:24, 8.01MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:   6% 114M/1.98G [00:14<02:28, 12.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:   5% 96.8M/1.94G [00:15<07:07, 4.30MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   5% 102M/1.94G [00:15<03:31, 8.67MB/s] \u001b[A\n","model-00002-of-00004.safetensors:   5% 105M/1.94G [00:15<03:04, 9.93MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:   6% 128M/1.98G [00:15<01:52, 16.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:   8% 142M/1.76G [00:15<02:01, 13.3MB/s]\n","\n","model-00001-of-00004.safetensors:   6% 123M/1.96G [00:15<04:09, 7.36MB/s]\u001b[A\u001b[A\n","\n","model-00004-of-00004.safetensors:   8% 146M/1.76G [00:16<02:18, 11.7MB/s]\n","model-00004-of-00004.safetensors:   9% 153M/1.76G [00:16<01:39, 16.1MB/s]\n","\n","model-00001-of-00004.safetensors:   6% 127M/1.96G [00:16<04:16, 7.14MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:   6% 109M/1.94G [00:16<05:01, 6.07MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:   7% 140M/1.98G [00:16<02:32, 12.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:   6% 111M/1.94G [00:16<04:05, 7.43MB/s]\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:   9% 157M/1.76G [00:17<03:00, 8.92MB/s]\n","\n","model-00001-of-00004.safetensors:   7% 128M/1.96G [00:17<06:44, 4.52MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:   7% 134M/1.96G [00:17<03:51, 7.87MB/s]\u001b[A\u001b[A\n","model-00004-of-00004.safetensors:   9% 159M/1.76G [00:17<02:47, 9.59MB/s]\n","\n","model-00001-of-00004.safetensors:   7% 137M/1.96G [00:17<03:07, 9.72MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:   6% 117M/1.94G [00:17<04:02, 7.51MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   6% 121M/1.94G [00:17<03:01, 10.0MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:   7% 145M/1.98G [00:17<04:12, 7.29MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:   8% 159M/1.98G [00:17<01:57, 15.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:   8% 165M/1.98G [00:18<02:07, 14.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:   9% 162M/1.76G [00:18<04:18, 6.20MB/s]\n","\n","model-00004-of-00004.safetensors:  10% 175M/1.76G [00:18<01:51, 14.2MB/s]\n","\n","model-00004-of-00004.safetensors:  10% 180M/1.76G [00:19<01:59, 13.2MB/s]\n","model-00004-of-00004.safetensors:  10% 185M/1.76G [00:19<01:38, 16.0MB/s]\n","model-00002-of-00004.safetensors:   6% 125M/1.94G [00:19<06:14, 4.84MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   7% 127M/1.94G [00:19<05:17, 5.70MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:   9% 173M/1.98G [00:19<03:12, 9.39MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:   7% 144M/1.96G [00:19<06:42, 4.51MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:   8% 158M/1.96G [00:19<02:11, 13.6MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  11% 189M/1.76G [00:20<02:50, 9.24MB/s]\n","\n","model-00001-of-00004.safetensors:   8% 163M/1.96G [00:20<02:28, 12.1MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:   9% 168M/1.96G [00:20<01:56, 15.3MB/s]\u001b[A\u001b[A\n","model-00004-of-00004.safetensors:  11% 192M/1.76G [00:20<02:37, 9.97MB/s]\n","\n","model-00001-of-00004.safetensors:   9% 172M/1.96G [00:20<01:43, 17.2MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:   7% 133M/1.94G [00:20<04:18, 6.98MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   7% 136M/1.94G [00:20<03:15, 9.23MB/s]\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  12% 207M/1.76G [00:21<01:46, 14.6MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  10% 192M/1.98G [00:21<02:51, 10.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:   7% 140M/1.94G [00:21<04:53, 6.12MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  10% 199M/1.98G [00:21<02:07, 14.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:   9% 175M/1.96G [00:21<03:31, 8.44MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:   7% 141M/1.94G [00:21<04:45, 6.29MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  13% 224M/1.76G [00:22<01:31, 16.9MB/s]\n","\n","\n","\n","model-00004-of-00004.safetensors:  13% 231M/1.76G [00:22<01:11, 21.4MB/s]\n","\n","model-00001-of-00004.safetensors:   9% 178M/1.96G [00:22<05:01, 5.91MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:   9% 184M/1.96G [00:22<03:14, 9.14MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  10% 205M/1.98G [00:22<02:59, 9.89MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:   7% 145M/1.94G [00:22<07:17, 4.10MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  10% 187M/1.96G [00:22<02:46, 10.7MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:   8% 157M/1.94G [00:22<02:12, 13.4MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  10% 207M/1.98G [00:22<03:05, 9.59MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:   8% 162M/1.94G [00:23<02:29, 11.9MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  13% 236M/1.76G [00:23<02:04, 12.3MB/s]\n","model-00002-of-00004.safetensors:   9% 171M/1.94G [00:23<01:41, 17.5MB/s]\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  14% 239M/1.76G [00:23<02:04, 12.2MB/s]\n","\n","model-00001-of-00004.safetensors:  10% 190M/1.96G [00:23<04:29, 6.57MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  11% 215M/1.98G [00:23<02:46, 10.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  10% 192M/1.96G [00:24<04:20, 6.79MB/s]\u001b[A\u001b[A\n","model-00004-of-00004.safetensors:  14% 242M/1.76G [00:24<03:19, 7.63MB/s]\n","\n","\n","\n","model-00004-of-00004.safetensors:  14% 255M/1.76G [00:24<01:35, 15.7MB/s]\n","\n","model-00001-of-00004.safetensors:  10% 194M/1.96G [00:24<06:17, 4.67MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  11% 222M/1.98G [00:25<03:51, 7.62MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  10% 199M/1.96G [00:25<03:49, 7.66MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  15% 265M/1.76G [00:25<01:36, 15.5MB/s]\n","\n","model-00001-of-00004.safetensors:  10% 203M/1.96G [00:25<04:37, 6.33MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  11% 225M/1.98G [00:26<05:54, 4.95MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  12% 239M/1.98G [00:26<01:58, 14.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  10% 205M/1.96G [00:26<04:31, 6.45MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  11% 206M/1.96G [00:26<04:21, 6.70MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  15% 269M/1.76G [00:26<02:47, 8.95MB/s]\n","\n","\n","\n","model-00004-of-00004.safetensors:  15% 272M/1.76G [00:26<02:38, 9.43MB/s]\n","\n","model-00001-of-00004.safetensors:  11% 208M/1.96G [00:27<06:35, 4.43MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  11% 223M/1.96G [00:27<01:58, 14.7MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:   9% 177M/1.94G [00:27<09:10, 3.20MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  10% 190M/1.94G [00:27<03:49, 7.62MB/s]\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  16% 274M/1.76G [00:27<03:49, 6.48MB/s]\n","\n","model-00004-of-00004.safetensors:  16% 287M/1.76G [00:27<01:42, 14.4MB/s]\n","\n","model-00001-of-00004.safetensors:  12% 231M/1.96G [00:28<01:57, 14.6MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  13% 256M/1.98G [00:28<03:03, 9.39MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  10% 195M/1.94G [00:28<03:34, 8.14MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  12% 235M/1.96G [00:28<01:43, 16.7MB/s]\u001b[A\u001b[A\n","model-00004-of-00004.safetensors:  17% 292M/1.76G [00:28<02:07, 11.6MB/s]\n","model-00004-of-00004.safetensors:  17% 300M/1.76G [00:28<01:25, 17.1MB/s]\n","model-00002-of-00004.safetensors:  11% 216M/1.94G [00:28<01:43, 16.6MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  12% 224M/1.94G [00:28<01:17, 22.0MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  13% 258M/1.98G [00:28<04:36, 6.23MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  13% 264M/1.98G [00:29<02:59, 9.55MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  17% 305M/1.76G [00:29<01:39, 14.7MB/s]\n","model-00004-of-00004.safetensors:  18% 317M/1.76G [00:29<01:00, 24.1MB/s]\n","\n","model-00001-of-00004.safetensors:  12% 238M/1.96G [00:29<03:41, 7.76MB/s]\u001b[A\u001b[A\n","model-00004-of-00004.safetensors:  19% 336M/1.76G [00:29<00:47, 29.9MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  14% 269M/1.98G [00:30<04:21, 6.56MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  14% 271M/1.98G [00:30<04:00, 7.12MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00004-of-00004.safetensors:  19% 343M/1.76G [00:30<00:59, 23.8MB/s]\n","model-00002-of-00004.safetensors:  12% 237M/1.94G [00:30<02:49, 10.0MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  12% 244M/1.96G [00:30<04:02, 7.06MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  13% 251M/1.96G [00:30<02:31, 11.3MB/s]\u001b[A\u001b[A\n","model-00004-of-00004.safetensors:  20% 352M/1.76G [00:30<01:08, 20.7MB/s]\n","\n","model-00004-of-00004.safetensors:  20% 359M/1.76G [00:31<00:55, 25.4MB/s]\n","\n","model-00001-of-00004.safetensors:  14% 266M/1.96G [00:31<01:22, 20.6MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  14% 273M/1.98G [00:31<06:50, 4.17MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  14% 286M/1.98G [00:31<02:16, 12.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  12% 242M/1.94G [00:31<04:16, 6.62MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  14% 272M/1.96G [00:31<01:43, 16.4MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  13% 250M/1.94G [00:31<02:20, 12.0MB/s]\u001b[A\n","\n","model-00004-of-00004.safetensors:  21% 364M/1.76G [00:32<01:39, 14.1MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  15% 291M/1.98G [00:32<02:28, 11.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  13% 256M/1.94G [00:32<02:25, 11.6MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  15% 296M/1.98G [00:32<01:57, 14.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  14% 263M/1.94G [00:32<01:40, 16.7MB/s]\u001b[A\n","\n","model-00004-of-00004.safetensors:  21% 368M/1.76G [00:32<01:47, 13.0MB/s]\n","\n","model-00001-of-00004.safetensors:  15% 299M/1.96G [00:32<00:57, 28.8MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  16% 305M/1.96G [00:32<01:17, 21.3MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  16% 319M/1.96G [00:33<00:48, 33.6MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  14% 267M/1.94G [00:33<02:38, 10.5MB/s]\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  22% 383M/1.76G [00:33<01:26, 15.9MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  15% 303M/1.98G [00:33<03:06, 8.99MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  14% 270M/1.94G [00:33<02:51, 9.74MB/s]\u001b[A\n","\n","model-00004-of-00004.safetensors:  22% 388M/1.76G [00:33<01:39, 13.9MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  15% 306M/1.98G [00:34<04:21, 6.41MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  23% 400M/1.76G [00:34<01:23, 16.4MB/s]\n","model-00002-of-00004.safetensors:  14% 273M/1.94G [00:34<04:19, 6.43MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  17% 332M/1.96G [00:34<01:51, 14.6MB/s]\u001b[A\u001b[A\n","model-00004-of-00004.safetensors:  23% 409M/1.76G [00:34<01:02, 21.8MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  16% 325M/1.98G [00:34<01:57, 14.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  17% 336M/1.96G [00:35<02:07, 12.7MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  15% 288M/1.94G [00:35<02:21, 11.6MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  16% 303M/1.94G [00:35<01:11, 22.8MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  17% 336M/1.98G [00:35<01:47, 15.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  18% 351M/1.98G [00:35<01:02, 26.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00004-of-00004.safetensors:  23% 413M/1.76G [00:35<01:58, 11.4MB/s]\n","\n","model-00001-of-00004.safetensors:  17% 339M/1.96G [00:35<03:09, 8.53MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  18% 343M/1.96G [00:36<02:31, 10.7MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  17% 320M/1.94G [00:36<01:26, 18.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  17% 333M/1.94G [00:36<00:56, 28.3MB/s]\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  24% 431M/1.76G [00:36<01:24, 15.9MB/s]\n","model-00002-of-00004.safetensors:  18% 340M/1.94G [00:36<01:09, 22.9MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  18% 347M/1.96G [00:36<03:21, 8.00MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  18% 352M/1.94G [00:37<00:49, 32.3MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  18% 363M/1.98G [00:37<01:55, 14.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  18% 349M/1.96G [00:37<03:14, 8.28MB/s]\u001b[A\u001b[A\n","\n","model-00004-of-00004.safetensors:  25% 442M/1.76G [00:37<01:18, 16.8MB/s]\n","model-00002-of-00004.safetensors:  19% 359M/1.94G [00:37<01:19, 19.9MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  18% 352M/1.96G [00:38<05:18, 5.04MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  19% 367M/1.96G [00:38<01:47, 14.8MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  19% 368M/1.94G [00:38<01:23, 18.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  20% 378M/1.94G [00:38<01:01, 25.4MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  18% 367M/1.98G [00:38<03:20, 8.06MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  19% 372M/1.96G [00:38<01:58, 13.4MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  20% 384M/1.94G [00:39<01:16, 20.3MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  25% 446M/1.76G [00:39<02:58, 7.40MB/s]\n","\n","model-00001-of-00004.safetensors:  20% 384M/1.96G [00:39<01:38, 16.0MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  20% 397M/1.96G [00:39<01:02, 25.2MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  19% 369M/1.98G [00:39<04:28, 6.00MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  19% 380M/1.98G [00:39<02:26, 10.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  20% 397M/1.94G [00:40<01:48, 14.1MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  21% 403M/1.96G [00:40<01:18, 19.9MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  19% 385M/1.98G [00:40<02:30, 10.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  21% 415M/1.96G [00:40<00:53, 29.0MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  26% 461M/1.76G [00:40<02:01, 10.7MB/s]\n","\n","model-00004-of-00004.safetensors:  27% 473M/1.76G [00:41<01:29, 14.3MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  20% 395M/1.98G [00:41<02:41, 9.80MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  21% 400M/1.94G [00:41<02:57, 8.68MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  21% 408M/1.94G [00:41<02:01, 12.6MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  20% 398M/1.98G [00:41<02:53, 9.15MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  22% 427M/1.96G [00:41<01:55, 13.2MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  22% 431M/1.96G [00:42<02:07, 12.0MB/s]\u001b[A\u001b[A\n","model-00004-of-00004.safetensors:  27% 478M/1.76G [00:42<02:25, 8.84MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  20% 401M/1.98G [00:42<04:12, 6.27MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  21% 407M/1.98G [00:42<02:43, 9.62MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  21% 416M/1.94G [00:42<02:54, 8.73MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  22% 433M/1.96G [00:43<03:07, 8.14MB/s]\u001b[A\u001b[A\n","\n","model-00004-of-00004.safetensors:  28% 496M/1.76G [00:43<01:31, 13.8MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  21% 412M/1.98G [00:43<03:24, 7.67MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  22% 418M/1.94G [00:43<04:03, 6.25MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  22% 431M/1.94G [00:43<01:53, 13.3MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  23% 454M/1.96G [00:43<01:44, 14.4MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  29% 511M/1.76G [00:44<01:06, 18.9MB/s]\n","model-00002-of-00004.safetensors:  22% 436M/1.94G [00:44<01:57, 12.8MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  24% 464M/1.96G [00:44<01:33, 16.0MB/s]\u001b[A\u001b[A\n","\n","model-00004-of-00004.safetensors:  29% 517M/1.76G [00:44<01:13, 16.9MB/s]\n","model-00002-of-00004.safetensors:  23% 448M/1.94G [00:44<01:33, 15.9MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  21% 416M/1.98G [00:44<05:01, 5.20MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  24% 461M/1.94G [00:44<00:58, 25.1MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  21% 424M/1.98G [00:44<02:45, 9.43MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  25% 486M/1.96G [00:45<01:08, 21.6MB/s]\u001b[A\u001b[A\n","\n","model-00004-of-00004.safetensors:  31% 543M/1.76G [00:45<00:43, 28.1MB/s]\n","model-00002-of-00004.safetensors:  24% 468M/1.94G [00:45<01:13, 20.0MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  22% 432M/1.98G [00:45<02:19, 11.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  31% 550M/1.76G [00:45<00:53, 22.6MB/s]\n","model-00002-of-00004.safetensors:  25% 480M/1.94G [00:45<01:09, 21.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  26% 495M/1.94G [00:46<00:46, 31.2MB/s]\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  32% 560M/1.76G [00:46<01:04, 18.6MB/s]\n","model-00004-of-00004.safetensors:  32% 570M/1.76G [00:46<00:47, 25.0MB/s]\n","model-00002-of-00004.safetensors:  26% 508M/1.94G [00:46<00:52, 27.4MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  23% 464M/1.98G [00:46<01:25, 17.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  33% 576M/1.76G [00:47<00:55, 21.3MB/s]\n","model-00002-of-00004.safetensors:  26% 513M/1.94G [00:47<01:05, 21.8MB/s]\u001b[A\n","\n","model-00004-of-00004.safetensors:  33% 589M/1.76G [00:47<00:36, 32.0MB/s]\n","model-00002-of-00004.safetensors:  27% 521M/1.94G [00:47<00:50, 28.1MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  26% 500M/1.96G [00:47<02:28, 9.81MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  27% 527M/1.94G [00:47<00:45, 31.2MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  26% 506M/1.96G [00:47<01:57, 12.4MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  25% 486M/1.98G [00:47<01:06, 22.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  34% 597M/1.76G [00:47<00:48, 23.8MB/s]\n","model-00004-of-00004.safetensors:  34% 602M/1.76G [00:47<00:45, 25.8MB/s]\n","\n","model-00001-of-00004.safetensors:  26% 512M/1.96G [00:47<01:53, 12.8MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  28% 543M/1.94G [00:47<00:41, 33.2MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  27% 520M/1.96G [00:48<01:19, 18.1MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  25% 499M/1.98G [00:48<01:13, 20.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  28% 549M/1.94G [00:48<01:00, 23.0MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  27% 528M/1.96G [00:48<01:25, 16.7MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  28% 542M/1.96G [00:48<00:49, 28.4MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  26% 512M/1.98G [00:48<01:09, 21.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  27% 526M/1.98G [00:48<00:46, 31.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  29% 560M/1.94G [00:49<01:05, 21.1MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  34% 607M/1.76G [00:49<01:36, 12.0MB/s]\n","\n","model-00001-of-00004.safetensors:  28% 549M/1.96G [00:49<01:03, 22.1MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  27% 532M/1.98G [00:49<01:01, 23.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  29% 560M/1.96G [00:49<01:07, 20.6MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  29% 565M/1.96G [00:49<00:59, 23.4MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  27% 544M/1.98G [00:49<01:04, 22.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  28% 558M/1.98G [00:50<00:43, 32.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00004-of-00004.safetensors:  35% 611M/1.76G [00:50<02:14, 8.60MB/s]\n","model-00002-of-00004.safetensors:  30% 576M/1.94G [00:50<01:43, 13.1MB/s]\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  36% 631M/1.76G [00:50<01:13, 15.4MB/s]\n","model-00002-of-00004.safetensors:  30% 579M/1.94G [00:51<02:39, 8.52MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  29% 571M/1.96G [00:51<02:12, 10.5MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  30% 585M/1.94G [00:51<01:53, 11.9MB/s]\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  36% 635M/1.76G [00:51<01:42, 11.1MB/s]\n","\n","model-00004-of-00004.safetensors:  36% 638M/1.76G [00:51<01:40, 11.2MB/s]\n","\n","\n","\n","model-00004-of-00004.safetensors:  36% 640M/1.76G [00:52<01:42, 10.9MB/s]\n","model-00002-of-00004.safetensors:  30% 588M/1.94G [00:52<02:47, 8.04MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  30% 590M/1.94G [00:52<02:39, 8.47MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  31% 592M/1.94G [00:52<02:35, 8.68MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  29% 577M/1.96G [00:52<03:28, 6.64MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  29% 578M/1.98G [00:52<02:36, 8.99MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  30% 587M/1.96G [00:53<01:57, 11.7MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  36% 642M/1.76G [00:53<02:51, 6.54MB/s]\n","\n","\n","\n","model-00004-of-00004.safetensors:  37% 646M/1.76G [00:53<02:08, 8.67MB/s]\n","\n","model-00001-of-00004.safetensors:  30% 592M/1.96G [00:53<02:03, 11.1MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  30% 596M/1.98G [00:53<01:31, 15.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  31% 601M/1.96G [00:53<01:20, 16.9MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  31% 605M/1.98G [00:53<01:01, 22.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  31% 593M/1.94G [00:53<04:23, 5.11MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  31% 606M/1.96G [00:53<01:09, 19.5MB/s]\u001b[A\u001b[A\n","model-00004-of-00004.safetensors:  38% 666M/1.76G [00:53<00:58, 18.9MB/s]\n","model-00002-of-00004.safetensors:  31% 601M/1.94G [00:53<02:08, 10.4MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  31% 610M/1.98G [00:54<01:15, 18.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  31% 624M/1.98G [00:54<00:42, 31.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  31% 611M/1.96G [00:54<01:26, 15.6MB/s]\u001b[A\u001b[A\n","\n","model-00004-of-00004.safetensors:  39% 688M/1.76G [00:54<00:37, 28.4MB/s]\n","\n","\n","\n","model-00004-of-00004.safetensors:  39% 694M/1.76G [00:55<00:47, 22.7MB/s]\n","model-00002-of-00004.safetensors:  31% 603M/1.94G [00:55<04:31, 4.92MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  32% 620M/1.96G [00:55<02:07, 10.5MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  32% 640M/1.98G [00:55<01:12, 18.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  31% 605M/1.94G [00:55<04:22, 5.08MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  32% 623M/1.96G [00:55<02:07, 10.5MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  40% 704M/1.76G [00:55<00:51, 20.6MB/s]\n","model-00004-of-00004.safetensors:  41% 722M/1.76G [00:56<00:45, 22.9MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  33% 652M/1.98G [00:56<01:35, 14.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  32% 625M/1.96G [00:56<03:26, 6.47MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  33% 639M/1.96G [00:56<01:23, 15.7MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  31% 608M/1.94G [00:56<06:13, 3.56MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  42% 736M/1.76G [00:56<00:43, 23.6MB/s]\n","\n","\n","\n","model-00004-of-00004.safetensors:  42% 746M/1.76G [00:56<00:34, 29.7MB/s]\n","\n","model-00001-of-00004.safetensors:  33% 645M/1.96G [00:57<01:31, 14.3MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  32% 624M/1.94G [00:57<02:06, 10.4MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  43% 767M/1.76G [00:57<00:28, 34.7MB/s]\n","\n","model-00001-of-00004.safetensors:  33% 656M/1.96G [00:57<01:21, 15.9MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  34% 671M/1.96G [00:57<00:47, 26.8MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  33% 658M/1.98G [00:57<02:56, 7.50MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  33% 646M/1.94G [00:58<01:18, 16.5MB/s]\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  44% 774M/1.76G [00:58<00:41, 23.7MB/s]\n","model-00002-of-00004.safetensors:  34% 652M/1.94G [00:58<01:02, 20.5MB/s]\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  44% 779M/1.76G [00:58<00:37, 26.2MB/s]\n","\n","model-00001-of-00004.safetensors:  35% 679M/1.96G [00:58<01:00, 21.1MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  34% 657M/1.94G [00:58<01:14, 17.2MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  34% 675M/1.98G [00:58<01:43, 12.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00004-of-00004.safetensors:  44% 784M/1.76G [00:58<00:47, 20.4MB/s]\n","\n","\n","\n","model-00004-of-00004.safetensors:  45% 791M/1.76G [00:58<00:38, 25.0MB/s]\n","\n","model-00001-of-00004.safetensors:  35% 688M/1.96G [00:58<01:04, 19.7MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  36% 703M/1.96G [00:59<00:41, 30.1MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  35% 673M/1.94G [00:59<01:07, 18.8MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  46% 816M/1.76G [00:59<00:27, 34.7MB/s]\n","\n","model-00001-of-00004.safetensors:  36% 710M/1.96G [00:59<00:52, 23.7MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  36% 694M/1.94G [00:59<00:51, 24.0MB/s]\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  47% 823M/1.76G [01:00<00:38, 24.6MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  35% 688M/1.98G [01:00<02:15, 9.52MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  37% 720M/1.96G [01:00<01:01, 20.2MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  37% 725M/1.96G [01:00<00:54, 22.7MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  36% 704M/1.94G [01:00<00:59, 20.7MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  48% 847M/1.76G [01:00<00:29, 31.3MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  35% 690M/1.98G [01:01<03:27, 6.22MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  37% 726M/1.94G [01:01<00:51, 23.8MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  35% 703M/1.98G [01:01<01:30, 14.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00004-of-00004.safetensors:  48% 854M/1.76G [01:01<00:36, 24.8MB/s]\n","\n","model-00001-of-00004.safetensors:  37% 731M/1.96G [01:01<01:28, 13.9MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  38% 735M/1.96G [01:01<01:29, 13.7MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  36% 707M/1.98G [01:01<01:40, 12.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  36% 720M/1.98G [01:01<00:56, 22.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00004-of-00004.safetensors:  49% 864M/1.76G [01:01<00:40, 22.1MB/s]\n","model-00004-of-00004.safetensors:  49% 870M/1.76G [01:01<00:35, 25.2MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  37% 726M/1.98G [01:02<01:08, 18.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  39% 759M/1.94G [01:02<00:48, 24.3MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  37% 736M/1.98G [01:02<01:06, 18.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  38% 750M/1.98G [01:02<00:41, 29.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00004-of-00004.safetensors:  50% 875M/1.76G [01:02<01:01, 14.5MB/s]\n","model-00002-of-00004.safetensors:  40% 768M/1.94G [01:02<00:56, 20.8MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  38% 742M/1.96G [01:03<02:04, 9.79MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  40% 781M/1.94G [01:03<00:38, 30.0MB/s]\u001b[A\n","\n","model-00004-of-00004.safetensors:  50% 879M/1.76G [01:03<01:01, 14.3MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  38% 757M/1.98G [01:03<00:53, 23.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  39% 768M/1.98G [01:03<00:38, 31.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  41% 787M/1.94G [01:03<00:53, 21.6MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  38% 748M/1.96G [01:03<02:43, 7.39MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  39% 774M/1.98G [01:04<00:53, 22.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  41% 800M/1.94G [01:04<00:50, 22.4MB/s]\u001b[A\n","\n","model-00004-of-00004.safetensors:  50% 889M/1.76G [01:04<01:08, 12.8MB/s]\n","model-00002-of-00004.safetensors:  42% 810M/1.94G [01:04<00:40, 28.1MB/s]\u001b[A\n","\n","model-00004-of-00004.safetensors:  51% 895M/1.76G [01:04<00:52, 16.4MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  40% 784M/1.98G [01:04<00:58, 20.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  40% 799M/1.98G [01:04<00:37, 31.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00004-of-00004.safetensors:  51% 899M/1.76G [01:04<01:01, 14.2MB/s]\n","model-00004-of-00004.safetensors:  51% 905M/1.76G [01:05<00:46, 18.5MB/s]\n","\n","model-00001-of-00004.safetensors:  38% 753M/1.96G [01:05<04:09, 4.83MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  39% 758M/1.96G [01:05<02:28, 8.09MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  41% 806M/1.98G [01:05<00:50, 23.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  39% 762M/1.96G [01:05<01:44, 11.4MB/s]\u001b[A\u001b[A\n","model-00004-of-00004.safetensors:  52% 912M/1.76G [01:05<00:53, 16.1MB/s]\n","model-00004-of-00004.safetensors:  52% 919M/1.76G [01:05<00:39, 21.6MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  41% 816M/1.98G [01:05<00:54, 21.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  42% 832M/1.98G [01:05<00:35, 32.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00004-of-00004.safetensors:  53% 928M/1.76G [01:06<00:44, 18.9MB/s]\n","\n","model-00004-of-00004.safetensors:  53% 943M/1.76G [01:06<00:25, 31.8MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  42% 839M/1.98G [01:06<00:44, 25.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  39% 766M/1.96G [01:06<02:54, 6.82MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  45% 864M/1.94G [01:06<00:47, 22.7MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  54% 949M/1.76G [01:06<00:34, 23.3MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  43% 848M/1.98G [01:07<00:51, 22.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  44% 863M/1.98G [01:07<00:33, 33.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00004-of-00004.safetensors:  54% 960M/1.76G [01:07<00:37, 21.4MB/s]\n","\n","model-00004-of-00004.safetensors:  55% 974M/1.76G [01:07<00:24, 32.5MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  44% 870M/1.98G [01:07<00:43, 25.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  39% 772M/1.96G [01:07<02:58, 6.63MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  44% 879M/1.98G [01:07<00:34, 31.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  40% 776M/1.96G [01:07<02:07, 9.26MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  46% 896M/1.94G [01:07<00:44, 23.6MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  56% 982M/1.76G [01:08<00:29, 26.6MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  45% 886M/1.98G [01:08<00:44, 24.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  40% 778M/1.96G [01:08<02:43, 7.24MB/s]\u001b[A\u001b[A\n","model-00004-of-00004.safetensors:  57% 1.00G/1.76G [01:08<00:25, 30.3MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  45% 896M/1.98G [01:08<00:51, 21.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  46% 911M/1.98G [01:08<00:33, 32.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  48% 928M/1.94G [01:09<00:43, 23.0MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  58% 1.02G/1.76G [01:09<00:21, 34.2MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  46% 918M/1.98G [01:09<00:42, 25.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  40% 780M/1.96G [01:09<05:10, 3.79MB/s]\u001b[A\u001b[A\n","model-00004-of-00004.safetensors:  58% 1.03G/1.76G [01:09<00:27, 26.4MB/s]\n","\n","model-00001-of-00004.safetensors:  40% 782M/1.96G [01:09<04:32, 4.32MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  47% 928M/1.98G [01:10<00:46, 22.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  48% 943M/1.98G [01:10<00:30, 33.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00004-of-00004.safetensors:  59% 1.04G/1.76G [01:10<00:29, 24.3MB/s]\n","model-00004-of-00004.safetensors:  60% 1.06G/1.76G [01:10<00:19, 35.5MB/s]\n","\n","model-00001-of-00004.safetensors:  40% 784M/1.96G [01:10<05:35, 3.50MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  48% 950M/1.98G [01:10<00:45, 22.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  50% 979M/1.94G [01:10<00:40, 23.8MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  40% 792M/1.96G [01:10<02:19, 8.36MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  48% 956M/1.98G [01:10<00:39, 26.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  41% 796M/1.96G [01:10<01:52, 10.3MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  51% 985M/1.94G [01:11<00:35, 26.9MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  41% 799M/1.96G [01:11<01:34, 12.3MB/s]\u001b[A\u001b[A\n","model-00004-of-00004.safetensors:  60% 1.07G/1.76G [01:11<00:29, 23.9MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  49% 962M/1.98G [01:11<00:51, 19.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  41% 802M/1.96G [01:11<01:55, 10.0MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  51% 995M/1.94G [01:11<00:43, 21.7MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  49% 972M/1.98G [01:11<00:35, 28.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  52% 1.00G/1.94G [01:11<00:35, 26.7MB/s]\u001b[A\n","\n","model-00004-of-00004.safetensors:  61% 1.07G/1.76G [01:11<00:36, 19.2MB/s]\n","model-00002-of-00004.safetensors:  52% 1.01G/1.94G [01:11<00:30, 30.7MB/s]\u001b[A\n","\n","model-00004-of-00004.safetensors:  61% 1.08G/1.76G [01:11<00:29, 23.2MB/s]\n","\n","model-00004-of-00004.safetensors:  61% 1.08G/1.76G [01:11<00:24, 27.7MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  49% 979M/1.98G [01:12<00:46, 21.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00004-of-00004.safetensors:  62% 1.09G/1.76G [01:12<00:31, 21.7MB/s]\n","model-00002-of-00004.safetensors:  53% 1.02G/1.94G [01:12<00:28, 31.7MB/s]\u001b[A\n","\n","model-00004-of-00004.safetensors:  62% 1.10G/1.76G [01:12<00:23, 28.5MB/s]\n","\n","model-00001-of-00004.safetensors:  42% 822M/1.96G [01:12<00:59, 19.1MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  42% 828M/1.96G [01:12<00:44, 25.7MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  50% 992M/1.98G [01:12<00:43, 22.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  51% 1.01G/1.98G [01:12<00:28, 33.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00004-of-00004.safetensors:  63% 1.10G/1.76G [01:12<00:28, 23.2MB/s]\n","model-00004-of-00004.safetensors:  63% 1.11G/1.76G [01:12<00:21, 29.8MB/s]\n","\n","model-00001-of-00004.safetensors:  42% 832M/1.96G [01:13<01:10, 15.9MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  43% 846M/1.96G [01:13<00:34, 31.8MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  64% 1.12G/1.76G [01:13<00:28, 22.7MB/s]\n","model-00004-of-00004.safetensors:  64% 1.13G/1.76G [01:13<00:22, 28.9MB/s]\n","model-00004-of-00004.safetensors:  64% 1.13G/1.76G [01:13<00:19, 33.1MB/s]\n","\n","model-00001-of-00004.safetensors:  44% 852M/1.96G [01:13<00:52, 21.0MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  52% 1.02G/1.98G [01:13<00:41, 22.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  44% 861M/1.96G [01:13<00:39, 28.1MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  65% 1.14G/1.76G [01:14<00:26, 23.9MB/s]\n","\n","model-00001-of-00004.safetensors:  44% 867M/1.96G [01:14<00:49, 22.0MB/s]\u001b[A\u001b[A\n","\n","model-00004-of-00004.safetensors:  66% 1.17G/1.76G [01:14<00:16, 35.2MB/s]\n","\n","model-00004-of-00004.safetensors:  67% 1.17G/1.76G [01:15<00:22, 26.0MB/s]\n","\n","model-00001-of-00004.safetensors:  46% 896M/1.96G [01:15<00:50, 21.2MB/s]\u001b[A\u001b[A\n","\n","model-00004-of-00004.safetensors:  68% 1.20G/1.76G [01:15<00:15, 36.0MB/s]\n","\n","model-00004-of-00004.safetensors:  68% 1.21G/1.76G [01:16<00:21, 26.2MB/s]\n","model-00002-of-00004.safetensors:  55% 1.06G/1.94G [01:16<02:15, 6.49MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  47% 928M/1.96G [01:16<00:45, 22.6MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  55% 1.06G/1.94G [01:16<01:41, 8.64MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  48% 935M/1.96G [01:16<00:38, 26.8MB/s]\u001b[A\u001b[A\n","\n","model-00004-of-00004.safetensors:  69% 1.22G/1.76G [01:17<00:24, 22.3MB/s]\n","\n","\n","\n","model-00004-of-00004.safetensors:  70% 1.23G/1.76G [01:17<00:15, 33.6MB/s]\n","model-00002-of-00004.safetensors:  55% 1.07G/1.94G [01:17<01:28, 9.77MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  53% 1.05G/1.98G [01:17<01:44, 8.94MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  56% 1.08G/1.94G [01:17<01:00, 14.2MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  53% 1.05G/1.98G [01:17<01:20, 11.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  56% 1.09G/1.94G [01:17<00:48, 17.6MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  48% 948M/1.96G [01:17<00:51, 19.7MB/s]\u001b[A\u001b[A\n","\n","model-00004-of-00004.safetensors:  70% 1.24G/1.76G [01:17<00:20, 25.8MB/s]\n","model-00002-of-00004.safetensors:  56% 1.09G/1.94G [01:17<00:49, 17.2MB/s]\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  71% 1.25G/1.76G [01:17<00:16, 31.7MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  54% 1.06G/1.98G [01:17<01:07, 13.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  57% 1.10G/1.94G [01:17<00:41, 20.0MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  54% 1.07G/1.98G [01:18<00:50, 18.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00004-of-00004.safetensors:  71% 1.25G/1.76G [01:18<00:21, 24.1MB/s]\n","model-00002-of-00004.safetensors:  57% 1.10G/1.94G [01:18<00:48, 17.3MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  54% 1.07G/1.98G [01:18<00:58, 15.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  57% 1.11G/1.94G [01:18<00:34, 23.6MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  54% 1.08G/1.98G [01:18<00:43, 20.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  58% 1.12G/1.94G [01:18<00:27, 29.7MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  55% 1.09G/1.98G [01:18<00:33, 26.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  50% 976M/1.96G [01:18<00:49, 19.9MB/s]\u001b[A\u001b[A\n","\n","model-00004-of-00004.safetensors:  72% 1.26G/1.76G [01:18<00:23, 21.2MB/s]\n","model-00004-of-00004.safetensors:  72% 1.27G/1.76G [01:19<00:17, 27.5MB/s]\n","model-00002-of-00004.safetensors:  58% 1.13G/1.94G [01:19<00:27, 29.1MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  55% 1.09G/1.98G [01:19<00:45, 19.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  73% 1.28G/1.76G [01:19<00:22, 21.9MB/s]\n","model-00004-of-00004.safetensors:  73% 1.29G/1.76G [01:19<00:15, 30.4MB/s]\n","model-00002-of-00004.safetensors:  59% 1.14G/1.94G [01:19<00:27, 28.5MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  59% 1.15G/1.94G [01:19<00:24, 32.5MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  50% 988M/1.96G [01:19<01:08, 14.2MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  56% 1.10G/1.98G [01:19<00:49, 17.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  74% 1.30G/1.76G [01:20<00:21, 22.1MB/s]\n","\n","model-00001-of-00004.safetensors:  51% 992M/1.96G [01:20<01:14, 13.0MB/s]\u001b[A\u001b[A\n","model-00004-of-00004.safetensors:  74% 1.31G/1.76G [01:20<00:13, 32.6MB/s]\n","model-00002-of-00004.safetensors:  60% 1.16G/1.94G [01:20<00:32, 24.2MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  60% 1.17G/1.94G [01:20<00:23, 32.8MB/s]\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  75% 1.32G/1.76G [01:20<00:18, 23.9MB/s]\n","model-00002-of-00004.safetensors:  61% 1.17G/1.94G [01:20<00:36, 21.2MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  57% 1.14G/1.98G [01:21<00:39, 21.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  58% 1.15G/1.98G [01:21<00:25, 32.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  51% 994M/1.96G [01:21<01:58, 8.16MB/s]\u001b[A\u001b[A\n","\n","model-00004-of-00004.safetensors:  76% 1.34G/1.76G [01:21<00:12, 34.3MB/s]\n","model-00002-of-00004.safetensors:  61% 1.18G/1.94G [01:21<00:40, 18.7MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  58% 1.16G/1.98G [01:21<00:33, 24.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  62% 1.20G/1.94G [01:21<00:26, 28.5MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  59% 1.16G/1.98G [01:21<00:29, 27.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  51% 1.01G/1.96G [01:21<01:22, 11.6MB/s]\u001b[A\u001b[A\n","\n","model-00004-of-00004.safetensors:  77% 1.36G/1.76G [01:22<00:14, 29.1MB/s]\n","model-00002-of-00004.safetensors:  62% 1.20G/1.94G [01:22<00:33, 21.7MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  59% 1.17G/1.98G [01:22<00:37, 21.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  63% 1.22G/1.94G [01:22<00:21, 33.3MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  59% 1.18G/1.98G [01:22<00:29, 27.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00004-of-00004.safetensors:  77% 1.36G/1.76G [01:22<00:17, 22.7MB/s]\n","\n","model-00004-of-00004.safetensors:  78% 1.37G/1.76G [01:22<00:13, 29.8MB/s]\n","model-00002-of-00004.safetensors:  63% 1.22G/1.94G [01:22<00:29, 24.6MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  60% 1.18G/1.98G [01:22<00:37, 21.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  60% 1.20G/1.98G [01:23<00:23, 33.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  53% 1.04G/1.96G [01:23<00:43, 20.9MB/s]\u001b[A\u001b[A\n","\n","model-00004-of-00004.safetensors:  79% 1.39G/1.76G [01:23<00:12, 31.0MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  61% 1.20G/1.98G [01:23<00:33, 23.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00004-of-00004.safetensors:  79% 1.39G/1.76G [01:23<00:15, 23.2MB/s]\n","\n","model-00004-of-00004.safetensors:  79% 1.40G/1.76G [01:23<00:12, 30.0MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  61% 1.22G/1.98G [01:24<00:34, 22.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  62% 1.23G/1.98G [01:24<00:22, 33.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00004-of-00004.safetensors:  81% 1.42G/1.76G [01:24<00:10, 31.4MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  62% 1.24G/1.98G [01:24<00:29, 25.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  56% 1.09G/1.96G [01:24<00:42, 20.7MB/s]\u001b[A\u001b[A\n","\n","model-00004-of-00004.safetensors:  81% 1.43G/1.76G [01:25<00:14, 22.8MB/s]\n","model-00002-of-00004.safetensors:  64% 1.23G/1.94G [01:25<01:13, 9.57MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  64% 1.24G/1.94G [01:25<00:47, 14.6MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  63% 1.25G/1.98G [01:25<00:32, 22.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  64% 1.26G/1.98G [01:25<00:22, 32.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00004-of-00004.safetensors:  82% 1.45G/1.76G [01:25<00:10, 31.0MB/s]\n","model-00002-of-00004.safetensors:  65% 1.25G/1.94G [01:25<00:49, 13.9MB/s]\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  83% 1.47G/1.76G [01:26<00:08, 35.8MB/s]\n","model-00002-of-00004.safetensors:  65% 1.26G/1.94G [01:26<00:41, 16.4MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  66% 1.28G/1.94G [01:26<00:26, 25.0MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  65% 1.28G/1.98G [01:26<00:29, 23.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  84% 1.48G/1.76G [01:26<00:11, 25.3MB/s]\n","model-00002-of-00004.safetensors:  66% 1.29G/1.94G [01:27<00:31, 20.9MB/s]\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  84% 1.49G/1.76G [01:27<00:12, 22.1MB/s]\n","model-00004-of-00004.safetensors:  85% 1.50G/1.76G [01:27<00:07, 33.3MB/s]\n","model-00002-of-00004.safetensors:  67% 1.31G/1.94G [01:27<00:23, 26.8MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  66% 1.31G/1.98G [01:27<00:30, 22.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  86% 1.51G/1.76G [01:28<00:09, 26.5MB/s]\n","model-00002-of-00004.safetensors:  68% 1.31G/1.94G [01:28<00:28, 21.6MB/s]\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  86% 1.52G/1.76G [01:28<00:10, 22.3MB/s]\n","model-00004-of-00004.safetensors:  87% 1.53G/1.76G [01:28<00:06, 32.9MB/s]\n","model-00002-of-00004.safetensors:  69% 1.34G/1.94G [01:28<00:20, 29.9MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  57% 1.12G/1.96G [01:28<01:50, 7.57MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  58% 1.13G/1.96G [01:28<01:27, 9.48MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  68% 1.34G/1.98G [01:28<00:28, 22.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  68% 1.35G/1.98G [01:29<00:22, 27.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  58% 1.14G/1.96G [01:29<01:03, 13.0MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  87% 1.54G/1.76G [01:29<00:08, 25.3MB/s]\n","model-00002-of-00004.safetensors:  69% 1.34G/1.94G [01:29<00:26, 22.6MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  70% 1.36G/1.94G [01:29<00:17, 33.1MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  58% 1.14G/1.96G [01:29<01:04, 12.6MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  89% 1.57G/1.76G [01:29<00:05, 33.1MB/s]\n","model-00002-of-00004.safetensors:  70% 1.36G/1.94G [01:30<00:23, 24.5MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  59% 1.15G/1.96G [01:30<01:00, 13.4MB/s]\u001b[A\u001b[A\n","\n","model-00004-of-00004.safetensors:  89% 1.57G/1.76G [01:30<00:07, 26.0MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  69% 1.38G/1.98G [01:30<00:35, 17.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  71% 1.38G/1.94G [01:30<00:24, 23.0MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  70% 1.39G/1.98G [01:30<00:25, 23.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  71% 1.38G/1.94G [01:30<00:19, 27.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  72% 1.39G/1.94G [01:30<00:17, 31.4MB/s]\u001b[A\n","\n","model-00004-of-00004.safetensors:  90% 1.58G/1.76G [01:31<00:08, 21.2MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  70% 1.39G/1.98G [01:31<00:31, 18.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00004-of-00004.safetensors:  91% 1.60G/1.76G [01:31<00:05, 30.0MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  71% 1.40G/1.98G [01:31<00:26, 21.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  72% 1.40G/1.94G [01:31<00:21, 25.4MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  71% 1.40G/1.98G [01:31<00:24, 24.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  73% 1.41G/1.94G [01:31<00:16, 32.5MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  60% 1.18G/1.96G [01:31<00:41, 18.7MB/s]\u001b[A\u001b[A\n","\n","model-00004-of-00004.safetensors:  91% 1.60G/1.76G [01:31<00:06, 23.9MB/s]\n","model-00002-of-00004.safetensors:  73% 1.41G/1.94G [01:31<00:23, 22.5MB/s]\u001b[A\n","\n","model-00004-of-00004.safetensors:  92% 1.63G/1.76G [01:32<00:03, 33.5MB/s]\n","model-00002-of-00004.safetensors:  73% 1.42G/1.94G [01:32<00:24, 21.2MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  71% 1.41G/1.98G [01:32<00:52, 11.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  74% 1.44G/1.94G [01:32<00:14, 33.5MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  62% 1.22G/1.96G [01:32<00:34, 21.6MB/s]\u001b[A\u001b[A\n","\n","model-00004-of-00004.safetensors:  93% 1.64G/1.76G [01:32<00:04, 26.4MB/s]\n","model-00002-of-00004.safetensors:  75% 1.44G/1.94G [01:33<00:20, 24.2MB/s]\u001b[A\n","\n","model-00004-of-00004.safetensors:  93% 1.65G/1.76G [01:33<00:05, 22.7MB/s]\n","\n","\n","\n","model-00004-of-00004.safetensors:  94% 1.66G/1.76G [01:33<00:03, 30.9MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  71% 1.42G/1.98G [01:33<00:52, 10.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  75% 1.46G/1.94G [01:33<00:21, 22.4MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  72% 1.42G/1.98G [01:33<00:37, 14.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  75% 1.46G/1.94G [01:33<00:18, 25.5MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  64% 1.25G/1.96G [01:33<00:31, 22.6MB/s]\u001b[A\u001b[A\n","\n","model-00004-of-00004.safetensors:  94% 1.67G/1.76G [01:34<00:03, 24.6MB/s]\n","\n","\n","\n","model-00004-of-00004.safetensors:  95% 1.68G/1.76G [01:34<00:02, 34.7MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  72% 1.43G/1.98G [01:34<00:29, 18.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  76% 1.47G/1.94G [01:34<00:21, 21.9MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  96% 1.69G/1.76G [01:34<00:02, 26.0MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  73% 1.44G/1.98G [01:34<00:33, 16.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  73% 1.45G/1.98G [01:34<00:17, 29.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  64% 1.26G/1.96G [01:35<00:48, 14.5MB/s]\u001b[A\u001b[A\n","model-00004-of-00004.safetensors:  97% 1.71G/1.76G [01:35<00:01, 34.4MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  74% 1.46G/1.98G [01:35<00:23, 22.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  78% 1.50G/1.94G [01:35<00:18, 22.9MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  97% 1.72G/1.76G [01:35<00:01, 25.7MB/s]\n","\n","model-00001-of-00004.safetensors:  64% 1.26G/1.96G [01:35<01:08, 10.2MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  74% 1.47G/1.98G [01:36<00:25, 19.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  79% 1.53G/1.94G [01:36<00:15, 25.9MB/s]\u001b[A\n","\n","\n","\n","model-00004-of-00004.safetensors:  99% 1.74G/1.76G [01:36<00:00, 32.7MB/s]\n","\n","model-00001-of-00004.safetensors:  65% 1.27G/1.96G [01:37<01:39, 6.98MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  75% 1.48G/1.98G [01:37<00:34, 14.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00004-of-00004.safetensors:  99% 1.75G/1.76G [01:37<00:00, 24.8MB/s]\n","model-00004-of-00004.safetensors: 100% 1.76G/1.76G [01:37<00:00, 31.1MB/s]\n","\n","\n","\n","model-00003-of-00004.safetensors:  75% 1.49G/1.98G [01:37<00:39, 12.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  66% 1.28G/1.96G [01:37<00:53, 12.7MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  79% 1.54G/1.94G [01:37<00:27, 14.4MB/s]\u001b[A\n","\n","model-00004-of-00004.safetensors: 100% 1.76G/1.76G [01:38<00:00, 18.0MB/s]\n","\n","\n","model-00001-of-00004.safetensors:  66% 1.30G/1.96G [01:38<00:38, 17.1MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  67% 1.31G/1.96G [01:38<00:25, 25.4MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  75% 1.49G/1.98G [01:38<01:01, 8.06MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  79% 1.54G/1.94G [01:38<00:41, 9.58MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  75% 1.49G/1.98G [01:38<00:45, 10.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  80% 1.55G/1.94G [01:38<00:27, 14.4MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  76% 1.50G/1.98G [01:38<00:38, 12.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  67% 1.32G/1.96G [01:38<00:31, 20.2MB/s]\u001b[A\u001b[A\n","\n","\n","Upload 4 LFS files:  25% 1/4 [01:38<04:56, 98.80s/it]\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  80% 1.55G/1.94G [01:39<00:30, 12.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  81% 1.57G/1.94G [01:39<00:15, 24.4MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  68% 1.33G/1.96G [01:39<00:31, 19.8MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  69% 1.34G/1.96G [01:39<00:19, 30.9MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  76% 1.50G/1.98G [01:39<00:59, 8.03MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  81% 1.57G/1.94G [01:39<00:18, 19.6MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  76% 1.50G/1.98G [01:39<00:59, 7.99MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  69% 1.35G/1.96G [01:39<00:25, 23.7MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  69% 1.36G/1.96G [01:40<00:19, 31.1MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  70% 1.37G/1.96G [01:40<00:25, 23.1MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  81% 1.58G/1.94G [01:40<00:27, 13.1MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  76% 1.50G/1.98G [01:40<01:39, 4.80MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  77% 1.52G/1.98G [01:40<00:34, 13.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  82% 1.58G/1.94G [01:41<00:28, 12.4MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  70% 1.38G/1.96G [01:41<00:27, 21.4MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  71% 1.39G/1.96G [01:41<00:17, 32.5MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  77% 1.52G/1.98G [01:41<00:37, 12.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  71% 1.40G/1.96G [01:41<00:22, 25.1MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  78% 1.54G/1.98G [01:41<00:27, 16.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  82% 1.59G/1.94G [01:42<00:43, 8.06MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  78% 1.55G/1.98G [01:42<00:16, 26.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  82% 1.59G/1.94G [01:42<00:31, 10.8MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  72% 1.41G/1.96G [01:42<00:24, 22.2MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  73% 1.42G/1.96G [01:42<00:16, 33.2MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  79% 1.56G/1.98G [01:42<00:20, 21.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  83% 1.60G/1.94G [01:42<00:27, 12.5MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  83% 1.61G/1.94G [01:42<00:14, 22.0MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  79% 1.57G/1.98G [01:43<00:20, 20.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  73% 1.43G/1.96G [01:43<00:24, 22.0MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  80% 1.58G/1.98G [01:43<00:15, 26.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  73% 1.44G/1.96G [01:43<00:20, 25.7MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  84% 1.62G/1.94G [01:43<00:22, 14.2MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  74% 1.44G/1.96G [01:43<00:24, 21.3MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  80% 1.58G/1.98G [01:43<00:18, 21.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  84% 1.63G/1.94G [01:43<00:17, 18.0MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  74% 1.45G/1.96G [01:43<00:19, 26.0MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  80% 1.59G/1.98G [01:43<00:14, 26.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  74% 1.46G/1.96G [01:43<00:17, 29.4MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  81% 1.60G/1.98G [01:43<00:13, 29.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  75% 1.46G/1.96G [01:44<00:22, 22.0MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  81% 1.60G/1.98G [01:44<00:17, 21.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  75% 1.47G/1.96G [01:44<00:14, 33.7MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  81% 1.61G/1.98G [01:44<00:13, 27.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  75% 1.48G/1.96G [01:44<00:20, 23.1MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  84% 1.63G/1.94G [01:45<00:30, 9.97MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  82% 1.62G/1.98G [01:45<00:25, 14.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  76% 1.49G/1.96G [01:45<00:23, 20.2MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  76% 1.50G/1.96G [01:45<00:17, 26.3MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  82% 1.62G/1.98G [01:45<00:17, 20.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  77% 1.50G/1.96G [01:45<00:14, 30.5MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  84% 1.63G/1.94G [01:46<00:41, 7.34MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  82% 1.63G/1.98G [01:46<00:19, 18.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  83% 1.65G/1.98G [01:46<00:11, 29.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  77% 1.51G/1.96G [01:46<00:21, 20.6MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  78% 1.52G/1.96G [01:46<00:14, 30.9MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  85% 1.65G/1.94G [01:46<00:25, 11.3MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  83% 1.65G/1.98G [01:46<00:14, 23.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  86% 1.66G/1.94G [01:46<00:15, 18.1MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  84% 1.66G/1.98G [01:46<00:12, 26.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  78% 1.53G/1.96G [01:46<00:19, 22.5MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  86% 1.67G/1.94G [01:47<00:16, 16.1MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  84% 1.66G/1.98G [01:47<00:16, 19.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  85% 1.68G/1.98G [01:47<00:09, 32.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  78% 1.54G/1.96G [01:47<00:21, 19.8MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  79% 1.55G/1.96G [01:47<00:13, 30.9MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  85% 1.69G/1.98G [01:48<00:12, 24.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  79% 1.56G/1.96G [01:48<00:17, 23.5MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  86% 1.68G/1.94G [01:48<00:21, 12.4MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  87% 1.68G/1.94G [01:48<00:21, 11.9MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  86% 1.70G/1.98G [01:48<00:13, 21.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  86% 1.71G/1.98G [01:48<00:08, 31.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  80% 1.57G/1.96G [01:48<00:18, 20.9MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  81% 1.58G/1.96G [01:48<00:11, 31.6MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  87% 1.72G/1.98G [01:49<00:10, 24.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  81% 1.59G/1.96G [01:49<00:15, 24.0MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  87% 1.68G/1.94G [01:49<00:33, 7.79MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  87% 1.69G/1.94G [01:49<00:15, 15.3MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  87% 1.73G/1.98G [01:49<00:11, 22.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  88% 1.74G/1.98G [01:49<00:07, 33.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  82% 1.60G/1.96G [01:50<00:16, 21.7MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  82% 1.61G/1.96G [01:50<00:10, 31.9MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  88% 1.70G/1.94G [01:50<00:16, 14.1MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  88% 1.71G/1.94G [01:50<00:10, 21.3MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  88% 1.75G/1.98G [01:50<00:08, 25.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  83% 1.62G/1.96G [01:50<00:13, 24.3MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  89% 1.72G/1.94G [01:50<00:12, 17.5MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  89% 1.76G/1.98G [01:50<00:09, 23.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  90% 1.77G/1.98G [01:51<00:06, 33.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  83% 1.63G/1.96G [01:51<00:16, 20.2MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  89% 1.73G/1.94G [01:51<00:11, 17.9MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  84% 1.64G/1.96G [01:51<00:12, 25.7MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  90% 1.74G/1.94G [01:51<00:08, 23.3MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  84% 1.65G/1.96G [01:51<00:10, 29.4MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  90% 1.74G/1.94G [01:51<00:07, 27.7MB/s]\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  90% 1.78G/1.98G [01:51<00:08, 23.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  90% 1.79G/1.98G [01:51<00:06, 30.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  90% 1.75G/1.94G [01:52<00:08, 21.5MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  84% 1.65G/1.96G [01:52<00:13, 22.5MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  91% 1.76G/1.94G [01:52<00:06, 27.3MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  85% 1.66G/1.96G [01:52<00:10, 28.5MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  91% 1.80G/1.98G [01:52<00:08, 22.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  91% 1.76G/1.94G [01:52<00:08, 20.6MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  85% 1.67G/1.96G [01:52<00:13, 21.1MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  92% 1.77G/1.94G [01:52<00:05, 32.7MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  85% 1.67G/1.96G [01:52<00:11, 25.3MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  92% 1.78G/1.94G [01:53<00:06, 23.0MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  86% 1.68G/1.96G [01:53<00:14, 19.3MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  86% 1.69G/1.96G [01:53<00:08, 31.8MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  92% 1.79G/1.94G [01:53<00:06, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  93% 1.81G/1.94G [01:53<00:03, 33.6MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  87% 1.70G/1.96G [01:54<00:12, 20.0MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  94% 1.81G/1.94G [01:54<00:05, 24.6MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  87% 1.71G/1.96G [01:54<00:12, 19.7MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  88% 1.73G/1.96G [01:54<00:07, 30.6MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  94% 1.82G/1.94G [01:55<00:05, 22.3MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  95% 1.84G/1.94G [01:55<00:02, 33.6MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  89% 1.73G/1.96G [01:55<00:09, 24.8MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  95% 1.85G/1.94G [01:55<00:03, 26.9MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  89% 1.74G/1.96G [01:56<00:09, 21.5MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  90% 1.76G/1.96G [01:56<00:06, 32.5MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  96% 1.86G/1.94G [01:56<00:03, 23.4MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  97% 1.87G/1.94G [01:56<00:01, 35.2MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  90% 1.77G/1.96G [01:56<00:07, 25.0MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  97% 1.88G/1.94G [01:56<00:02, 27.0MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  91% 1.78G/1.96G [01:57<00:08, 21.9MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  91% 1.79G/1.96G [01:57<00:05, 32.0MB/s]\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  97% 1.89G/1.94G [01:57<00:02, 22.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  98% 1.90G/1.94G [01:57<00:01, 34.1MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  92% 1.80G/1.96G [01:57<00:06, 24.9MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  91% 1.81G/1.98G [01:57<00:35, 4.86MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  91% 1.81G/1.98G [01:57<00:28, 5.89MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  92% 1.82G/1.98G [01:58<00:17, 8.95MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  99% 1.91G/1.94G [01:58<00:01, 24.9MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  92% 1.81G/1.96G [01:58<00:06, 23.5MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  93% 1.82G/1.96G [01:58<00:03, 35.2MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  92% 1.83G/1.98G [01:58<00:16, 9.44MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","model-00002-of-00004.safetensors:  99% 1.92G/1.94G [01:58<00:00, 21.3MB/s]\u001b[A\n","model-00002-of-00004.safetensors: 100% 1.93G/1.94G [01:58<00:00, 31.8MB/s]\u001b[A\n","\n","model-00001-of-00004.safetensors:  93% 1.83G/1.96G [01:58<00:04, 27.1MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  93% 1.83G/1.98G [01:59<00:17, 8.37MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  94% 1.84G/1.96G [01:59<00:05, 22.8MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  95% 1.86G/1.96G [01:59<00:03, 34.3MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00002-of-00004.safetensors: 100% 1.94G/1.94G [01:59<00:00, 16.2MB/s]\n","\n","\n","model-00001-of-00004.safetensors:  95% 1.86G/1.96G [02:00<00:03, 26.5MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  96% 1.87G/1.96G [02:00<00:03, 22.1MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  96% 1.89G/1.96G [02:00<00:02, 32.6MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  93% 1.84G/1.98G [02:00<00:22, 6.21MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  93% 1.85G/1.98G [02:00<00:15, 8.66MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  97% 1.89G/1.96G [02:01<00:02, 24.3MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  93% 1.85G/1.98G [02:01<00:18, 7.11MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  94% 1.85G/1.98G [02:02<00:18, 7.10MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  94% 1.86G/1.98G [02:02<00:16, 7.84MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  97% 1.90G/1.96G [02:02<00:03, 15.1MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  97% 1.90G/1.96G [02:02<00:03, 14.2MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  94% 1.86G/1.98G [02:03<00:25, 4.87MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  94% 1.87G/1.98G [02:03<00:07, 14.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  97% 1.91G/1.96G [02:03<00:05, 9.86MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  95% 1.88G/1.98G [02:03<00:08, 12.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  98% 1.92G/1.96G [02:04<00:02, 13.3MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  99% 1.93G/1.96G [02:04<00:01, 21.5MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  95% 1.89G/1.98G [02:04<00:06, 15.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  96% 1.90G/1.98G [02:04<00:02, 26.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors:  99% 1.94G/1.96G [02:04<00:01, 18.4MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  96% 1.91G/1.98G [02:05<00:03, 20.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","model-00001-of-00004.safetensors: 100% 1.95G/1.96G [02:05<00:00, 19.3MB/s]\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  97% 1.92G/1.98G [02:05<00:03, 19.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00001-of-00004.safetensors: 100% 1.96G/1.96G [02:06<00:00, 15.5MB/s]\n","\n","\n","\n","\n","model-00003-of-00004.safetensors:  98% 1.94G/1.98G [02:06<00:01, 22.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","Upload 4 LFS files:  50% 2/4 [02:06<01:54, 57.07s/it]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  98% 1.95G/1.98G [02:07<00:02, 14.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  98% 1.95G/1.98G [02:07<00:02, 13.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  99% 1.95G/1.98G [02:08<00:03, 8.75MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors:  99% 1.97G/1.98G [02:08<00:00, 16.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","model-00003-of-00004.safetensors: 100% 1.98G/1.98G [02:09<00:00, 15.3MB/s]\n","\n","\n","\n","Upload 4 LFS files: 100% 4/4 [02:10<00:00, 32.60s/it]\n","[INFO|tokenization_utils_base.py:2488] 2024-05-14 16:37:31,214 >> tokenizer config file saved in phi_lora_merged/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2497] 2024-05-14 16:37:31,215 >> Special tokens file saved in phi_lora_merged/special_tokens_map.json\n","README.md: 100% 5.19k/5.19k [00:00<00:00, 25.2MB/s]\n","[INFO|tokenization_utils_base.py:2488] 2024-05-14 16:37:32,047 >> tokenizer config file saved in /tmp/tmpwotor1dp/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2497] 2024-05-14 16:37:32,047 >> Special tokens file saved in /tmp/tmpwotor1dp/special_tokens_map.json\n","[INFO|hub.py:757] 2024-05-14 16:37:32,095 >> Uploading the following files to bertilmuth/phi: special_tokens_map.json,tokenizer.model,README.md,added_tokens.json,tokenizer_config.json,tokenizer.json\n","tokenizer.model: 100% 500k/500k [00:00<00:00, 1.26MB/s]\n"]}],"source":["import json\n","\n","%cd /content/LLaMA-Factory\n","args = dict(\n","  model_name_or_path=hf_base_model_id,             # the hugging face model id\n","  adapter_name_or_path=adapter_name,            # load the saved LoRA adapters\n","  template=llamafactory_template_name,          # same to the one in training\n","  finetuning_type=\"lora\",                  # same to the one in training\n","  export_dir=saved_merged_model_path,              # the path to save the merged model\n","  export_size=2,                       # the file shard size (in GB) of the merged model\n","  export_device=\"cpu\",                    # the device used in export, can be chosen from `cpu` and `cuda`\n","  export_hub_model_id=hf_finetuned_model_id      # the Hugging Face hub ID to upload model\n",")\n","\n","json.dump(args, open(\"merge_file.json\", \"w\", encoding=\"utf-8\"), indent=2)\n","!llamafactory-cli export merge_file.json"]}],"metadata":{"colab":{"provenance":[{"file_id":"1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9","timestamp":1715161074742}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}